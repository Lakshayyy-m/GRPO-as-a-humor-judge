{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "83uFWNQIWuXT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "153c8198f62343278d1c4237e75bbd69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_969b7dffc94e48eda9d0452aed10c47c",
              "IPY_MODEL_c0d6da1a88834d598620e0b2574972eb",
              "IPY_MODEL_0a922d26b2a847e7858d6b561854f8dc"
            ],
            "layout": "IPY_MODEL_2026561fb04c4f95b107e5055471eebf"
          }
        },
        "969b7dffc94e48eda9d0452aed10c47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d87efaa5f43442997996a0bf0091d6e",
            "placeholder": "​",
            "style": "IPY_MODEL_f68f41ffaf4a4bf49c03f49e97fe4cd9",
            "value": "model.safetensors: 100%"
          }
        },
        "c0d6da1a88834d598620e0b2574972eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc2096cb310a43a9ac65d229a6f2307d",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_067a961a273740faafcd418c8f4ddfeb",
            "value": 3087466850
          }
        },
        "0a922d26b2a847e7858d6b561854f8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_760d0b673f4847e6ba9b4a5465f890e0",
            "placeholder": "​",
            "style": "IPY_MODEL_133b4a1d7c974f1bb6107af11cabcc39",
            "value": " 3.09G/3.09G [00:08&lt;00:00, 593MB/s]"
          }
        },
        "2026561fb04c4f95b107e5055471eebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d87efaa5f43442997996a0bf0091d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f68f41ffaf4a4bf49c03f49e97fe4cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc2096cb310a43a9ac65d229a6f2307d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "067a961a273740faafcd418c8f4ddfeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "760d0b673f4847e6ba9b4a5465f890e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "133b4a1d7c974f1bb6107af11cabcc39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98ad66c6eaa64fbeb67ba7eb0545c5d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3ca556ece884e99be1377dc47f108f4",
              "IPY_MODEL_28d0caaed25449b7ae1b1c2814507ce3",
              "IPY_MODEL_7fe526fecccf48098b69f8422214f7de"
            ],
            "layout": "IPY_MODEL_7fc5571ebb0442069337e6759e8d1010"
          }
        },
        "d3ca556ece884e99be1377dc47f108f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52c077bafec24a9fbd9292d54e5142f0",
            "placeholder": "​",
            "style": "IPY_MODEL_626d8f7072274b12bde8425270aaafc8",
            "value": "generation_config.json: 100%"
          }
        },
        "28d0caaed25449b7ae1b1c2814507ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14621f188a04b89bcbf37f2cf1ce26b",
            "max": 270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c90ce854f9840458edfea06b4032fd7",
            "value": 270
          }
        },
        "7fe526fecccf48098b69f8422214f7de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c79baad09ce74accb71aef1eec041a1f",
            "placeholder": "​",
            "style": "IPY_MODEL_5fde1309f9cd4f6eb1d0eea249e707b7",
            "value": " 270/270 [00:00&lt;00:00, 27.5kB/s]"
          }
        },
        "7fc5571ebb0442069337e6759e8d1010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c077bafec24a9fbd9292d54e5142f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "626d8f7072274b12bde8425270aaafc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c14621f188a04b89bcbf37f2cf1ce26b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c90ce854f9840458edfea06b4032fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c79baad09ce74accb71aef1eec041a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fde1309f9cd4f6eb1d0eea249e707b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a283a3b93204ccc92166b4bc4045972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e678d391e29f48bc84ceb0ee58f3cbab",
              "IPY_MODEL_66b99d1fe0f6477cb31074c7507c64c8",
              "IPY_MODEL_268e2bce43684bceac83c28d2e65080d"
            ],
            "layout": "IPY_MODEL_9706d3d2e125475d807c01f4dc200128"
          }
        },
        "e678d391e29f48bc84ceb0ee58f3cbab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95849ec200484876b50d658d192132df",
            "placeholder": "​",
            "style": "IPY_MODEL_e08ebbf785524f5b8c95fcc6316abe31",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "66b99d1fe0f6477cb31074c7507c64c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a7e534d649426599479ed1b5313fdd",
            "max": 7362,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ff22dc173714315a0723f5fc3e98f44",
            "value": 7362
          }
        },
        "268e2bce43684bceac83c28d2e65080d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d9bd82071334bf0ad6df4c104d6b681",
            "placeholder": "​",
            "style": "IPY_MODEL_36220da5f17e4b2bbe669c4440054f7d",
            "value": " 7.36k/7.36k [00:00&lt;00:00, 811kB/s]"
          }
        },
        "9706d3d2e125475d807c01f4dc200128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95849ec200484876b50d658d192132df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e08ebbf785524f5b8c95fcc6316abe31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a7e534d649426599479ed1b5313fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ff22dc173714315a0723f5fc3e98f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d9bd82071334bf0ad6df4c104d6b681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36220da5f17e4b2bbe669c4440054f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a2ae4807faf41e78871192bda2a7dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4838eef738b142759d415d0506130595",
              "IPY_MODEL_ffe31bf3faab44b2b2756808c38a3537",
              "IPY_MODEL_6f672698fa484b7d85dcfcb601031de2"
            ],
            "layout": "IPY_MODEL_692cbb723dea4b0c914758fa76aa1bb9"
          }
        },
        "4838eef738b142759d415d0506130595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff5d4c491e34ce591b61857532bf6f0",
            "placeholder": "​",
            "style": "IPY_MODEL_af2367d4b65f4de1b3fc2616abc3ff1f",
            "value": "merges.txt: 100%"
          }
        },
        "ffe31bf3faab44b2b2756808c38a3537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41982fa645e4a588883c6e515d0022c",
            "max": 1671853,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27ad58c86e134408a86580e39fd8ae59",
            "value": 1671853
          }
        },
        "6f672698fa484b7d85dcfcb601031de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6158dccb29f6486780163ab49431b933",
            "placeholder": "​",
            "style": "IPY_MODEL_5a1997e25fd04296928c7194ce09f636",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 46.8MB/s]"
          }
        },
        "692cbb723dea4b0c914758fa76aa1bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aff5d4c491e34ce591b61857532bf6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2367d4b65f4de1b3fc2616abc3ff1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f41982fa645e4a588883c6e515d0022c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ad58c86e134408a86580e39fd8ae59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6158dccb29f6486780163ab49431b933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1997e25fd04296928c7194ce09f636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d35ec8e1cb9f4bdab45e2b53a67403ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4746c2b580464ee18fabd57f49d4f9aa",
              "IPY_MODEL_258766cdc6fc478c81a7ad5963106148",
              "IPY_MODEL_ccc9a927c8e04523920e6245e44d99c2"
            ],
            "layout": "IPY_MODEL_7d401006898846a184ccd4bd6532651e"
          }
        },
        "4746c2b580464ee18fabd57f49d4f9aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c34a07e21c8a436aa21a9681091552fe",
            "placeholder": "​",
            "style": "IPY_MODEL_aedcfdfbd0804198b9eab6b90b7e761a",
            "value": "added_tokens.json: 100%"
          }
        },
        "258766cdc6fc478c81a7ad5963106148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac98ebbc7d6049dfb6fdb6dab5d026b6",
            "max": 605,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a68bffbed301487d9d2e1b5faa9a441f",
            "value": 605
          }
        },
        "ccc9a927c8e04523920e6245e44d99c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63e78a6edfdd4774acf388541e4264da",
            "placeholder": "​",
            "style": "IPY_MODEL_3e8e265b83e3452ea6cc20615a810272",
            "value": " 605/605 [00:00&lt;00:00, 79.6kB/s]"
          }
        },
        "7d401006898846a184ccd4bd6532651e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c34a07e21c8a436aa21a9681091552fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aedcfdfbd0804198b9eab6b90b7e761a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac98ebbc7d6049dfb6fdb6dab5d026b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a68bffbed301487d9d2e1b5faa9a441f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63e78a6edfdd4774acf388541e4264da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e8e265b83e3452ea6cc20615a810272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2bf99138b4a48e2957ed92b1d4a099b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa173602c1dd4eb88215055b16d1a7a5",
              "IPY_MODEL_5b1a673ebc244f5abf2b10df322b7ce1",
              "IPY_MODEL_4e36c74d47494590bea733aba84f8268"
            ],
            "layout": "IPY_MODEL_ada125bb6a264a888564109f776c3ca5"
          }
        },
        "aa173602c1dd4eb88215055b16d1a7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f5f568b66f84ceda20f97cfc3a49020",
            "placeholder": "​",
            "style": "IPY_MODEL_75f76ece87514458a5fea2f57fe0e572",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5b1a673ebc244f5abf2b10df322b7ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac7aa6329363483d8d83b496b63f3ac1",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86f2e23d0410406ab245def515d16601",
            "value": 614
          }
        },
        "4e36c74d47494590bea733aba84f8268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d74619888051428fa2031346288bb16e",
            "placeholder": "​",
            "style": "IPY_MODEL_d65517df934344839373ba2a0069dee0",
            "value": " 614/614 [00:00&lt;00:00, 75.7kB/s]"
          }
        },
        "ada125bb6a264a888564109f776c3ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5f568b66f84ceda20f97cfc3a49020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f76ece87514458a5fea2f57fe0e572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac7aa6329363483d8d83b496b63f3ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f2e23d0410406ab245def515d16601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d74619888051428fa2031346288bb16e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d65517df934344839373ba2a0069dee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96729d672ed54ebfb327d92d8bcd1c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27d4d99dc30d45fd9dee81ca5826867c",
              "IPY_MODEL_e053b1e6120847a28cec21c4722aec51",
              "IPY_MODEL_cf9e3a5cb9744a0aad51619c1ad67912"
            ],
            "layout": "IPY_MODEL_8e7b99f9933f4333a30baf5cd1ef5d8d"
          }
        },
        "27d4d99dc30d45fd9dee81ca5826867c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f32624536034203a10baa79e866eef4",
            "placeholder": "​",
            "style": "IPY_MODEL_4340de8d0f864623aa81831abb1c587d",
            "value": "tokenizer.json: 100%"
          }
        },
        "e053b1e6120847a28cec21c4722aec51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6029e267a624b35a6af4ed90cb17bee",
            "max": 11421896,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ccb78bdeebc42fb8a68b8c176d939d0",
            "value": 11421896
          }
        },
        "cf9e3a5cb9744a0aad51619c1ad67912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaa4033d6b5b4149b83202ca2c3ebc41",
            "placeholder": "​",
            "style": "IPY_MODEL_f5e36c247a4240d2a55c3372fabed9a5",
            "value": " 11.4M/11.4M [00:00&lt;00:00, 131MB/s]"
          }
        },
        "8e7b99f9933f4333a30baf5cd1ef5d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f32624536034203a10baa79e866eef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4340de8d0f864623aa81831abb1c587d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6029e267a624b35a6af4ed90cb17bee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ccb78bdeebc42fb8a68b8c176d939d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaa4033d6b5b4149b83202ca2c3ebc41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5e36c247a4240d2a55c3372fabed9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e98e00f33b0c42b9a4234fc84fe260b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58a68f7f81534fc09841c7ec22800eb2",
              "IPY_MODEL_36a99e4f54fc4bdc97e4ad70769ca5e7",
              "IPY_MODEL_519523d43e204485af1c606695620153"
            ],
            "layout": "IPY_MODEL_08b3010eb6be4dc5881e66ca152cd9c5"
          }
        },
        "58a68f7f81534fc09841c7ec22800eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6a480ff764e4e95b566d4c901eb3717",
            "placeholder": "​",
            "style": "IPY_MODEL_b9ba0a8afb674bc4805acc1aa545f02a",
            "value": "vocab.json: 100%"
          }
        },
        "36a99e4f54fc4bdc97e4ad70769ca5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edf26794d7e54ad4b990cfd67dfe8230",
            "max": 2776833,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7739c09cdae4e4dbd049321fe1f120e",
            "value": 2776833
          }
        },
        "519523d43e204485af1c606695620153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d436e8900dc4e03a4bca131c4fb231c",
            "placeholder": "​",
            "style": "IPY_MODEL_963b90e023374ccb932a24bbe8a82263",
            "value": " 2.78M/2.78M [00:00&lt;00:00, 6.21MB/s]"
          }
        },
        "08b3010eb6be4dc5881e66ca152cd9c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a480ff764e4e95b566d4c901eb3717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9ba0a8afb674bc4805acc1aa545f02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edf26794d7e54ad4b990cfd67dfe8230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7739c09cdae4e4dbd049321fe1f120e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d436e8900dc4e03a4bca131c4fb231c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963b90e023374ccb932a24bbe8a82263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c1fc2421d284fdd995258efbc6a3b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_161022ddf28e48ab8f4d77171b4e10f0",
              "IPY_MODEL_6f7cb86c8f3644159627fb2f0f41b141",
              "IPY_MODEL_c0f412baf8d743be91b60ef7d0faffdb"
            ],
            "layout": "IPY_MODEL_b7321a1144544612b5881be0e41989cf"
          }
        },
        "161022ddf28e48ab8f4d77171b4e10f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a816d0c03aa8416bb06537fef753dfcd",
            "placeholder": "​",
            "style": "IPY_MODEL_e73c571991d74b429bf1e6611ba44a4d",
            "value": "Map: 100%"
          }
        },
        "6f7cb86c8f3644159627fb2f0f41b141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32f01a78831c460bbe10469cf77682b5",
            "max": 488,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9658a4452d346298308ef8fc26a3384",
            "value": 488
          }
        },
        "c0f412baf8d743be91b60ef7d0faffdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b2e7ef0d274bb1956845f4352b1154",
            "placeholder": "​",
            "style": "IPY_MODEL_e637c7f8b96a43728b7af57dd2a7b21e",
            "value": " 488/488 [00:00&lt;00:00, 7204.20 examples/s]"
          }
        },
        "b7321a1144544612b5881be0e41989cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a816d0c03aa8416bb06537fef753dfcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e73c571991d74b429bf1e6611ba44a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32f01a78831c460bbe10469cf77682b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9658a4452d346298308ef8fc26a3384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1b2e7ef0d274bb1956845f4352b1154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e637c7f8b96a43728b7af57dd2a7b21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc4a60e41b6e44f592b7e0d9bf9ef766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cacb1a59e364097a01e53166fba7c8e",
              "IPY_MODEL_d346bbff74a24dcf892979567d6d63ff",
              "IPY_MODEL_b737a39835c04b58b6a6e2b364a93c5e"
            ],
            "layout": "IPY_MODEL_e623280f08044cfe935eb880f94ba3d2"
          }
        },
        "1cacb1a59e364097a01e53166fba7c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c5c1fa0560e4cb08c0fbe68189a7450",
            "placeholder": "​",
            "style": "IPY_MODEL_f19d6e3045c5419e94720b0a7acf7fa0",
            "value": "Evaluating: 100%"
          }
        },
        "d346bbff74a24dcf892979567d6d63ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58af688595e34b2ebc3ca6258abd08bd",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_991ef15ba63a4c599c3da1889ebe2a90",
            "value": 49
          }
        },
        "b737a39835c04b58b6a6e2b364a93c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d27f5906ba7348c689b8779928d885ee",
            "placeholder": "​",
            "style": "IPY_MODEL_384ca2001bf64bd080fcb5e418693804",
            "value": " 49/49 [02:56&lt;00:00,  3.73s/it]"
          }
        },
        "e623280f08044cfe935eb880f94ba3d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c5c1fa0560e4cb08c0fbe68189a7450": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19d6e3045c5419e94720b0a7acf7fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58af688595e34b2ebc3ca6258abd08bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991ef15ba63a4c599c3da1889ebe2a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d27f5906ba7348c689b8779928d885ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384ca2001bf64bd080fcb5e418693804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm"
      ],
      "metadata": {
        "id": "9PhgiQvR35IH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import random # For A/B assignment\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import re\n",
        "import numpy as np\n",
        "import random # Ensure random is imported"
      ],
      "metadata": {
        "id": "zOGmqo6MPbIK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "vv9wWoCg4evn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "153c8198f62343278d1c4237e75bbd69",
            "969b7dffc94e48eda9d0452aed10c47c",
            "c0d6da1a88834d598620e0b2574972eb",
            "0a922d26b2a847e7858d6b561854f8dc",
            "2026561fb04c4f95b107e5055471eebf",
            "7d87efaa5f43442997996a0bf0091d6e",
            "f68f41ffaf4a4bf49c03f49e97fe4cd9",
            "cc2096cb310a43a9ac65d229a6f2307d",
            "067a961a273740faafcd418c8f4ddfeb",
            "760d0b673f4847e6ba9b4a5465f890e0",
            "133b4a1d7c974f1bb6107af11cabcc39",
            "98ad66c6eaa64fbeb67ba7eb0545c5d1",
            "d3ca556ece884e99be1377dc47f108f4",
            "28d0caaed25449b7ae1b1c2814507ce3",
            "7fe526fecccf48098b69f8422214f7de",
            "7fc5571ebb0442069337e6759e8d1010",
            "52c077bafec24a9fbd9292d54e5142f0",
            "626d8f7072274b12bde8425270aaafc8",
            "c14621f188a04b89bcbf37f2cf1ce26b",
            "7c90ce854f9840458edfea06b4032fd7",
            "c79baad09ce74accb71aef1eec041a1f",
            "5fde1309f9cd4f6eb1d0eea249e707b7",
            "4a283a3b93204ccc92166b4bc4045972",
            "e678d391e29f48bc84ceb0ee58f3cbab",
            "66b99d1fe0f6477cb31074c7507c64c8",
            "268e2bce43684bceac83c28d2e65080d",
            "9706d3d2e125475d807c01f4dc200128",
            "95849ec200484876b50d658d192132df",
            "e08ebbf785524f5b8c95fcc6316abe31",
            "71a7e534d649426599479ed1b5313fdd",
            "6ff22dc173714315a0723f5fc3e98f44",
            "2d9bd82071334bf0ad6df4c104d6b681",
            "36220da5f17e4b2bbe669c4440054f7d",
            "8a2ae4807faf41e78871192bda2a7dcd",
            "4838eef738b142759d415d0506130595",
            "ffe31bf3faab44b2b2756808c38a3537",
            "6f672698fa484b7d85dcfcb601031de2",
            "692cbb723dea4b0c914758fa76aa1bb9",
            "aff5d4c491e34ce591b61857532bf6f0",
            "af2367d4b65f4de1b3fc2616abc3ff1f",
            "f41982fa645e4a588883c6e515d0022c",
            "27ad58c86e134408a86580e39fd8ae59",
            "6158dccb29f6486780163ab49431b933",
            "5a1997e25fd04296928c7194ce09f636",
            "d35ec8e1cb9f4bdab45e2b53a67403ec",
            "4746c2b580464ee18fabd57f49d4f9aa",
            "258766cdc6fc478c81a7ad5963106148",
            "ccc9a927c8e04523920e6245e44d99c2",
            "7d401006898846a184ccd4bd6532651e",
            "c34a07e21c8a436aa21a9681091552fe",
            "aedcfdfbd0804198b9eab6b90b7e761a",
            "ac98ebbc7d6049dfb6fdb6dab5d026b6",
            "a68bffbed301487d9d2e1b5faa9a441f",
            "63e78a6edfdd4774acf388541e4264da",
            "3e8e265b83e3452ea6cc20615a810272",
            "a2bf99138b4a48e2957ed92b1d4a099b",
            "aa173602c1dd4eb88215055b16d1a7a5",
            "5b1a673ebc244f5abf2b10df322b7ce1",
            "4e36c74d47494590bea733aba84f8268",
            "ada125bb6a264a888564109f776c3ca5",
            "6f5f568b66f84ceda20f97cfc3a49020",
            "75f76ece87514458a5fea2f57fe0e572",
            "ac7aa6329363483d8d83b496b63f3ac1",
            "86f2e23d0410406ab245def515d16601",
            "d74619888051428fa2031346288bb16e",
            "d65517df934344839373ba2a0069dee0",
            "96729d672ed54ebfb327d92d8bcd1c2b",
            "27d4d99dc30d45fd9dee81ca5826867c",
            "e053b1e6120847a28cec21c4722aec51",
            "cf9e3a5cb9744a0aad51619c1ad67912",
            "8e7b99f9933f4333a30baf5cd1ef5d8d",
            "4f32624536034203a10baa79e866eef4",
            "4340de8d0f864623aa81831abb1c587d",
            "e6029e267a624b35a6af4ed90cb17bee",
            "0ccb78bdeebc42fb8a68b8c176d939d0",
            "aaa4033d6b5b4149b83202ca2c3ebc41",
            "f5e36c247a4240d2a55c3372fabed9a5",
            "e98e00f33b0c42b9a4234fc84fe260b9",
            "58a68f7f81534fc09841c7ec22800eb2",
            "36a99e4f54fc4bdc97e4ad70769ca5e7",
            "519523d43e204485af1c606695620153",
            "08b3010eb6be4dc5881e66ca152cd9c5",
            "c6a480ff764e4e95b566d4c901eb3717",
            "b9ba0a8afb674bc4805acc1aa545f02a",
            "edf26794d7e54ad4b990cfd67dfe8230",
            "b7739c09cdae4e4dbd049321fe1f120e",
            "1d436e8900dc4e03a4bca131c4fb231c",
            "963b90e023374ccb932a24bbe8a82263"
          ]
        },
        "id": "te3DcgPf1qh0",
        "outputId": "5723a6f1-1156-4da5-9a60-e0927c800fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "153c8198f62343278d1c4237e75bbd69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98ad66c6eaa64fbeb67ba7eb0545c5d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a283a3b93204ccc92166b4bc4045972"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a2ae4807faf41e78871192bda2a7dcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d35ec8e1cb9f4bdab45e2b53a67403ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2bf99138b4a48e2957ed92b1d4a099b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96729d672ed54ebfb327d92d8bcd1c2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e98e00f33b0c42b9a4234fc84fe260b9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "\n",
        "TARGET_NOTES_LENGTH = 128\n",
        "\n",
        "\n",
        "max_seq_length = 1536\n",
        "lora_rank = 32\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-1.5B-Instruct\", # Or \"Qwen/Qwen1.5-7B-Chat\", \"mistralai/Mistral-7B-Instruct-v0.2\" etc.\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=None,\n",
        "    load_in_4bit=False,\n",
        "    max_lora_rank=lora_rank,\n",
        ")\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_rank,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
        "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
        "\n",
        "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
        "\n",
        "For example:\n",
        "\n",
        "JUDGE: <notes>\n",
        "Sample A is superior to Sample B... (example notes)\n",
        "</notes>\n",
        "<judgement>A</judgement>\n",
        "\n",
        "Now, it is your turn.\"\"\"\n"
      ],
      "metadata": {
        "id": "l7fMPfyh8nB7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('/content/humor_classification_results.csv')\n",
        "df.dropna(subset=['abstract', 'original_title', 'created_title', 'humor_category'], inplace=True)\n",
        "\n",
        "\n",
        "df['target_label'] = 'original_title'\n",
        "raw_dataset = Dataset.from_pandas(df)\n",
        "print(f\"Loaded {len(raw_dataset)} examples from /content/humor_classification_results.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BpwMkWxpP_Bf",
        "outputId": "fc6675b5-dcbd-4d55-c1c9-3b1db323c85b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 488 examples from /content/humor_classification_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def format_title_AB_prompt(example):\n",
        "    original = example['original_title']\n",
        "    created = example['created_title']\n",
        "    target_label = example['target_label']\n",
        "    assignment = {}\n",
        "    target_choice = None\n",
        "\n",
        "    if random.random() < 0.5:\n",
        "\n",
        "        sample_a_content = original\n",
        "        sample_b_content = created\n",
        "        assignment['A'] = 'original_title'\n",
        "        assignment['B'] = 'created_title'\n",
        "        target_choice = 'B' if target_label == 'created_title' else 'A'\n",
        "    else:\n",
        "        sample_a_content = created\n",
        "        sample_b_content = original\n",
        "        assignment['A'] = 'created_title'\n",
        "        assignment['B'] = 'original_title'\n",
        "        target_choice = 'A' if target_label == 'created_title' else 'B'\n",
        "\n",
        "    user_content = f\"\"\"Abstract:\n",
        "{example['abstract']}\n",
        "\n",
        "[Sample A]:\n",
        "{sample_a_content}\n",
        "\n",
        "[Sample B]:\n",
        "{sample_b_content}\n",
        "\n",
        "JUDGE:\"\"\"\n",
        "\n",
        "    prompt_list = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ]\n",
        "\n",
        "    example['prompt'] = prompt_list\n",
        "    example['assignment'] = assignment\n",
        "    example['target_choice'] = target_choice\n",
        "\n",
        "    return example\n",
        "\n",
        "dataset = raw_dataset.map(format_title_AB_prompt)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3c1fc2421d284fdd995258efbc6a3b6a",
            "161022ddf28e48ab8f4d77171b4e10f0",
            "6f7cb86c8f3644159627fb2f0f41b141",
            "c0f412baf8d743be91b60ef7d0faffdb",
            "b7321a1144544612b5881be0e41989cf",
            "a816d0c03aa8416bb06537fef753dfcd",
            "e73c571991d74b429bf1e6611ba44a4d",
            "32f01a78831c460bbe10469cf77682b5",
            "c9658a4452d346298308ef8fc26a3384",
            "d1b2e7ef0d274bb1956845f4352b1154",
            "e637c7f8b96a43728b7af57dd2a7b21e"
          ]
        },
        "id": "h0_I-WcW5Qgx",
        "outputId": "8596d106-9980-4d8e-bd9a-c7b7055effad"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/488 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c1fc2421d284fdd995258efbc6a3b6a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Example Formatted Data ---\")\n",
        "print(tokenizer.apply_chat_template(dataset[4]['prompt'], tokenize=False, add_generation_prompt=True)) # Assumes tokenizer is defined\n",
        "print(f\"Assignment: {dataset[4]['assignment']}\")\n",
        "print(f\"Target Label: {dataset[4]['target_label']}\")\n",
        "print(f\"==> Target Choice for Rewards: {dataset[4]['target_choice']}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Zx99Pz24M544",
        "outputId": "0491bdbb-af67-405d-8a07-d0d723443eaa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example Formatted Data ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora. In this role they play an important part in making those resources usable for a wider audience. Over the past decades, several such query systems and languages have emerged, varying greatly in their expressiveness and technical details. This paper offers a broad overview of the history of corpora and corpus query tools. It focusses strongly on the query side and hints at exciting directions for future development.\n",
            "\n",
            "[Sample A]:\n",
            "A Survey of Corpus Query Systems and Their Development\n",
            "\n",
            "[Sample B]:\n",
            "To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Assignment: {'A': 'created_title', 'B': 'original_title'}\n",
            "Target Label: original_title\n",
            "==> Target Choice for Rewards: B\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tags(text: str) -> tuple[str | None, str | None]:\n",
        "    notes_match = re.search(r\"<notes>(.*?)</notes>\", text, re.DOTALL | re.IGNORECASE)\n",
        "    judgement_match = re.search(r\"<judgement>\\s*([AB])\\s*</judgement>\", text, re.IGNORECASE)\n",
        "\n",
        "    notes = notes_match.group(1).strip() if notes_match else None\n",
        "    judgement = judgement_match.group(1).strip().upper() if judgement_match else None\n",
        "    return notes, judgement\n",
        "\n",
        "print_counter = 0\n",
        "PRINT_EVERY_N_STEPS = 5\n",
        "\n",
        "def judgement_reward_func(prompts, completions, target_choice, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Calculates reward based on judgement correctness.\n",
        "    Also prints prompt and first completion periodically during training.\n",
        "    \"\"\"\n",
        "    global print_counter\n",
        "\n",
        "\n",
        "    if not target_choice or not target_choice[0] in ['A', 'B']:\n",
        "        print(f\"Warning: Invalid target_choice in judgement_reward_func: {target_choice}\")\n",
        "        return [0.0] * len(completions)\n",
        "    target = target_choice[0]\n",
        "\n",
        "    rewards = []\n",
        "    log_this_step = (print_counter % PRINT_EVERY_N_STEPS == 0)\n",
        "\n",
        "\n",
        "    if log_this_step and prompts:\n",
        "        try:\n",
        "            prompt_text = tokenizer.apply_chat_template(\n",
        "                prompts[0],\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False\n",
        "            )\n",
        "            print(\"\\n\" + \"=\"*20 + f\" Training Step Log (Print Step {print_counter}) \" + \"=\"*20)\n",
        "            print(\"\\n--- Prompt ---\")\n",
        "            print(prompt_text)\n",
        "\n",
        "            if completions and completions[0]:\n",
        "                 first_completion_text = completions[0][0][\"content\"]\n",
        "                 print(\"\\n--- Generated Response [0] ---\")\n",
        "                 print(first_completion_text)\n",
        "            else:\n",
        "                 print(\"\\n--- Generated Response [0]: (Not available) ---\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError during logging in reward function: {e}\")\n",
        "\n",
        "    for i, completion in enumerate(completions):\n",
        "        current_reward = 0.0\n",
        "        try:\n",
        "            text = completion[0][\"content\"]\n",
        "            _, judgement = extract_tags(text)\n",
        "\n",
        "            if judgement == target:\n",
        "                current_reward = 1.0\n",
        "            else:\n",
        "                current_reward = -0.1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error calculating reward for completion {i}: {e}\")\n",
        "            current_reward = 0.0\n",
        "\n",
        "        rewards.append(current_reward)\n",
        "\n",
        "    print_counter += 1\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def format_bonus_reward_func(completions, target_choice, **kwargs) -> list[float]:\n",
        "    target = target_choice[0]\n",
        "    rewards = []\n",
        "    format_pattern = r\"^\\s*<notes>.*?</notes>\\s*<judgement>\\s*[AB]\\s*</judgement>\\s*$\"\n",
        "\n",
        "    for i, completion in enumerate(completions):\n",
        "        text = completion[0][\"content\"] if isinstance(completion, list) else completion.get(\"content\", \"\")\n",
        "        notes, judgement = extract_tags(text)\n",
        "        matches_format = bool(re.match(format_pattern, text, re.DOTALL | re.IGNORECASE))\n",
        "\n",
        "        reward = 0.0\n",
        "        if matches_format:\n",
        "            reward = 0.2\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def length_bonus_reward_func_simplified(completions, target_choice, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Provides a reward bonus ONLY IF:\n",
        "    1. The generated judgement matches the target_choice.\n",
        "    2. The <notes> content exists.\n",
        "    3. The CHARACTER length of the <notes> content is within a tolerance\n",
        "       range of TARGET_NOTES_LENGTH (defined in characters).\n",
        "\n",
        "    The reward decreases linearly from 0.2 (at target length) to 0 (at tolerance edge).\n",
        "    \"\"\"\n",
        "\n",
        "    if not target_choice or not target_choice[0] in ['A', 'B']:\n",
        "        print(f\"Warning: Invalid target_choice received: {target_choice}\")\n",
        "        return [0.0] * len(completions)\n",
        "    target_judgement = target_choice[0]\n",
        "\n",
        "    rewards = []\n",
        "    target_len_chars = TARGET_NOTES_LENGTH\n",
        "    tolerance_ratio = 0.3\n",
        "    max_deviation = target_len_chars * tolerance_ratio\n",
        "    min_len_chars = target_len_chars - max_deviation\n",
        "    max_len_chars = target_len_chars + max_deviation\n",
        "    max_reward = 0.2\n",
        "\n",
        "    for completion in completions:\n",
        "        current_reward = 0.0\n",
        "        try:\n",
        "            text = completion[0][\"content\"]\n",
        "            notes, generated_judgement = extract_tags(text)\n",
        "\n",
        "            if generated_judgement == target_judgement:\n",
        "                if notes is not None:\n",
        "                    notes_char_length = len(notes)\n",
        "\n",
        "                    if min_len_chars <= notes_char_length <= max_len_chars:\n",
        "                        deviation = abs(notes_char_length - target_len_chars)\n",
        "\n",
        "\n",
        "                        if max_deviation > 0:\n",
        "                            current_reward = max_reward * (1 - (deviation / max_deviation))\n",
        "                        else:\n",
        "                            current_reward = max_reward if deviation == 0 else 0.0\n",
        "\n",
        "        except (IndexError, TypeError, KeyError, AttributeError) as e:\n",
        "            print(f\"Warning: Error processing completion: {e}. Completion: {completion}\")\n",
        "            current_reward = 0.0\n",
        "\n",
        "        rewards.append(current_reward)\n",
        "\n",
        "\n",
        "\n",
        "    return rewards\n"
      ],
      "metadata": {
        "id": "QTu2n2sg32Ec"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "import datasets\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"outputs_title_eval_kalomaze_corrected\",\n",
        "    num_train_epochs=1,\n",
        "    max_steps=300,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=5e-6,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"wandb\",\n",
        "    remove_unused_columns=False,\n",
        "    max_prompt_length= 1010,\n",
        "    num_generations=4,\n",
        "    max_completion_length=173,\n",
        "    beta=0.04,\n",
        "    max_grad_norm=0.2,\n",
        "    eval_steps=eval_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "fZV3VkjXTXWR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3943818b-fffd-462e-c711-2d054d52ea2d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        judgement_reward_func,\n",
        "        format_bonus_reward_func,\n",
        "        length_bonus_reward_func_simplified,\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Starting GRPO training (Corrected Params)...\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during training: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97460
        },
        "id": "a6HDh6BdTvja",
        "outputId": "da1fcf2b-84a9-41e8-c262-d2a17968c859"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GRPO training (Corrected Params)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 439 | Num Epochs = 3 | Total steps = 300\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 36,929,536/1,580,643,840 (2.34% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Training Step Log (Print Step 40) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.\n",
            "\n",
            "[Sample A]:\n",
            "Incorporating Unambiguous Word Annotations to Improve Coverage and Performance in Word Sense Disambiguation Tasks\n",
            "\n",
            "[Sample B]:\n",
            "Don't Neglect the Obvious: On the Role of Unambiguous Words in Word Sense Disambiguation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "Sample B is superior to Sample A... (Sample B is more engaging and highlights the focus on unambiguous words)\n",
            "\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 2:06:50, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / judgement_reward_func</th>\n",
              "      <th>rewards / format_bonus_reward_func</th>\n",
              "      <th>rewards / length_bonus_reward_func_simplified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.446875</td>\n",
              "      <td>0.585853</td>\n",
              "      <td>57.156250</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.558105</td>\n",
              "      <td>0.520741</td>\n",
              "      <td>46.062500</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.001855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.256250</td>\n",
              "      <td>0.367315</td>\n",
              "      <td>62.656250</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.412500</td>\n",
              "      <td>0.533812</td>\n",
              "      <td>53.375000</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>0.451942</td>\n",
              "      <td>57.468750</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.384375</td>\n",
              "      <td>0.551348</td>\n",
              "      <td>73.593750</td>\n",
              "      <td>0.001220</td>\n",
              "      <td>0.346875</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.450987</td>\n",
              "      <td>64.187500</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.018750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.419629</td>\n",
              "      <td>0.462661</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>0.000870</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.000879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.374702</td>\n",
              "      <td>66.750000</td>\n",
              "      <td>0.000983</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.434375</td>\n",
              "      <td>0.540838</td>\n",
              "      <td>55.093750</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.018750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.418750</td>\n",
              "      <td>0.504458</td>\n",
              "      <td>53.343750</td>\n",
              "      <td>0.001430</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.398568</td>\n",
              "      <td>0.497511</td>\n",
              "      <td>52.406250</td>\n",
              "      <td>0.000985</td>\n",
              "      <td>0.346875</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.001693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.612500</td>\n",
              "      <td>0.448002</td>\n",
              "      <td>50.875000</td>\n",
              "      <td>0.000748</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.314434</td>\n",
              "      <td>61.468750</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.446875</td>\n",
              "      <td>0.472539</td>\n",
              "      <td>56.187500</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.508671</td>\n",
              "      <td>53.093750</td>\n",
              "      <td>0.001352</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.081250</td>\n",
              "      <td>0.283931</td>\n",
              "      <td>65.562500</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.043750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.351086</td>\n",
              "      <td>59.281250</td>\n",
              "      <td>0.001026</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.056250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>0.521171</td>\n",
              "      <td>53.375000</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.481250</td>\n",
              "      <td>0.575474</td>\n",
              "      <td>52.468750</td>\n",
              "      <td>0.001393</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.165853</td>\n",
              "      <td>0.442215</td>\n",
              "      <td>69.468750</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>0.140625</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.000228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.574349</td>\n",
              "      <td>0.560023</td>\n",
              "      <td>56.625000</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.005599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.555273</td>\n",
              "      <td>0.540481</td>\n",
              "      <td>55.625000</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.005273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.459375</td>\n",
              "      <td>0.503619</td>\n",
              "      <td>53.906250</td>\n",
              "      <td>0.002574</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.043750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.412500</td>\n",
              "      <td>0.548278</td>\n",
              "      <td>65.718750</td>\n",
              "      <td>0.001775</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.340625</td>\n",
              "      <td>0.419567</td>\n",
              "      <td>64.500000</td>\n",
              "      <td>0.002239</td>\n",
              "      <td>0.278125</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.451037</td>\n",
              "      <td>55.562500</td>\n",
              "      <td>0.002092</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.068750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.571745</td>\n",
              "      <td>0.562614</td>\n",
              "      <td>56.687500</td>\n",
              "      <td>0.003655</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.002995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>0.356140</td>\n",
              "      <td>67.937500</td>\n",
              "      <td>0.005193</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.521018</td>\n",
              "      <td>59.687500</td>\n",
              "      <td>0.003731</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.056250</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.590625</td>\n",
              "      <td>0.467883</td>\n",
              "      <td>67.031250</td>\n",
              "      <td>0.004973</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.683594</td>\n",
              "      <td>0.465547</td>\n",
              "      <td>60.843750</td>\n",
              "      <td>0.012484</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.002344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.446549</td>\n",
              "      <td>0.596380</td>\n",
              "      <td>54.843750</td>\n",
              "      <td>0.010886</td>\n",
              "      <td>0.346875</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.005924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.440625</td>\n",
              "      <td>0.441978</td>\n",
              "      <td>56.968750</td>\n",
              "      <td>0.019933</td>\n",
              "      <td>0.346875</td>\n",
              "      <td>0.093750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.694108</td>\n",
              "      <td>0.459169</td>\n",
              "      <td>77.312500</td>\n",
              "      <td>0.017127</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.137500</td>\n",
              "      <td>0.003483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.560938</td>\n",
              "      <td>0.612757</td>\n",
              "      <td>52.875000</td>\n",
              "      <td>0.039203</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.106250</td>\n",
              "      <td>0.004688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.615007</td>\n",
              "      <td>0.498057</td>\n",
              "      <td>78.968750</td>\n",
              "      <td>0.021328</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>0.008757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.763900</td>\n",
              "      <td>0.556706</td>\n",
              "      <td>76.593750</td>\n",
              "      <td>0.020517</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.131250</td>\n",
              "      <td>0.010775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.470898</td>\n",
              "      <td>0.477682</td>\n",
              "      <td>77.343750</td>\n",
              "      <td>0.024034</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>0.002148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.495182</td>\n",
              "      <td>0.489384</td>\n",
              "      <td>67.437500</td>\n",
              "      <td>0.026027</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.007682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.508919</td>\n",
              "      <td>0.487297</td>\n",
              "      <td>78.781250</td>\n",
              "      <td>0.019591</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.002669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.574300</td>\n",
              "      <td>81.968750</td>\n",
              "      <td>0.022606</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.609277</td>\n",
              "      <td>0.586738</td>\n",
              "      <td>80.937500</td>\n",
              "      <td>0.025458</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.012402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.466504</td>\n",
              "      <td>0.446314</td>\n",
              "      <td>68.593750</td>\n",
              "      <td>0.046603</td>\n",
              "      <td>0.278125</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.000879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.666504</td>\n",
              "      <td>0.583964</td>\n",
              "      <td>81.031250</td>\n",
              "      <td>0.020563</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.000879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.699675</td>\n",
              "      <td>0.449350</td>\n",
              "      <td>79.281250</td>\n",
              "      <td>0.026933</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.005924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.506315</td>\n",
              "      <td>0.542320</td>\n",
              "      <td>81.718750</td>\n",
              "      <td>0.022984</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.000065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.863867</td>\n",
              "      <td>0.387529</td>\n",
              "      <td>84.468750</td>\n",
              "      <td>0.020814</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.007617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.576204</td>\n",
              "      <td>0.519110</td>\n",
              "      <td>74.250000</td>\n",
              "      <td>0.029787</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.001204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.582292</td>\n",
              "      <td>0.514378</td>\n",
              "      <td>75.125000</td>\n",
              "      <td>0.024828</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.001042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.614225</td>\n",
              "      <td>0.565299</td>\n",
              "      <td>72.625000</td>\n",
              "      <td>0.025161</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.011100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.684050</td>\n",
              "      <td>0.598060</td>\n",
              "      <td>79.656250</td>\n",
              "      <td>0.019902</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.005924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.675195</td>\n",
              "      <td>0.391978</td>\n",
              "      <td>78.531250</td>\n",
              "      <td>0.027447</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.003320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.608073</td>\n",
              "      <td>0.392530</td>\n",
              "      <td>75.750000</td>\n",
              "      <td>0.023666</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.004948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.613207</td>\n",
              "      <td>0.512879</td>\n",
              "      <td>83.642857</td>\n",
              "      <td>0.019186</td>\n",
              "      <td>0.410714</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.002493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.683236</td>\n",
              "      <td>0.544874</td>\n",
              "      <td>80.687500</td>\n",
              "      <td>0.017856</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.005111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.846257</td>\n",
              "      <td>0.377267</td>\n",
              "      <td>76.187500</td>\n",
              "      <td>0.030825</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.002507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.511979</td>\n",
              "      <td>0.443045</td>\n",
              "      <td>78.968750</td>\n",
              "      <td>0.024590</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.005729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.640397</td>\n",
              "      <td>0.511611</td>\n",
              "      <td>82.906250</td>\n",
              "      <td>0.022312</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.002897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.659375</td>\n",
              "      <td>0.524679</td>\n",
              "      <td>88.593750</td>\n",
              "      <td>0.020525</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.740625</td>\n",
              "      <td>0.425148</td>\n",
              "      <td>92.562500</td>\n",
              "      <td>0.029850</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.646419</td>\n",
              "      <td>0.430339</td>\n",
              "      <td>84.281250</td>\n",
              "      <td>0.024470</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.002669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.598698</td>\n",
              "      <td>0.519262</td>\n",
              "      <td>81.250000</td>\n",
              "      <td>0.021547</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.001823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.700391</td>\n",
              "      <td>0.539040</td>\n",
              "      <td>89.343750</td>\n",
              "      <td>0.037294</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.000391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.827018</td>\n",
              "      <td>0.508949</td>\n",
              "      <td>76.375000</td>\n",
              "      <td>0.029827</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.005143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.638216</td>\n",
              "      <td>0.613694</td>\n",
              "      <td>92.031250</td>\n",
              "      <td>0.021757</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.000716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.656478</td>\n",
              "      <td>0.474714</td>\n",
              "      <td>90.343750</td>\n",
              "      <td>0.026786</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.012728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.811393</td>\n",
              "      <td>0.539675</td>\n",
              "      <td>85.062500</td>\n",
              "      <td>0.095714</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.002018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.432385</td>\n",
              "      <td>75.218750</td>\n",
              "      <td>0.028684</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.615625</td>\n",
              "      <td>0.502521</td>\n",
              "      <td>89.062500</td>\n",
              "      <td>0.026164</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.545247</td>\n",
              "      <td>0.381130</td>\n",
              "      <td>81.750000</td>\n",
              "      <td>0.021788</td>\n",
              "      <td>0.346875</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.004622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.617155</td>\n",
              "      <td>0.584967</td>\n",
              "      <td>83.031250</td>\n",
              "      <td>0.019341</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.001530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.576270</td>\n",
              "      <td>0.520079</td>\n",
              "      <td>85.062500</td>\n",
              "      <td>0.021662</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.001270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.438884</td>\n",
              "      <td>83.687500</td>\n",
              "      <td>0.028803</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.568750</td>\n",
              "      <td>0.526086</td>\n",
              "      <td>82.031250</td>\n",
              "      <td>0.023538</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.719466</td>\n",
              "      <td>0.354876</td>\n",
              "      <td>77.562500</td>\n",
              "      <td>0.025798</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.748145</td>\n",
              "      <td>0.523293</td>\n",
              "      <td>79.187500</td>\n",
              "      <td>0.024774</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.007520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.866146</td>\n",
              "      <td>0.374177</td>\n",
              "      <td>80.406250</td>\n",
              "      <td>0.021685</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.009896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.671875</td>\n",
              "      <td>0.594836</td>\n",
              "      <td>92.968750</td>\n",
              "      <td>0.021189</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.598633</td>\n",
              "      <td>0.570560</td>\n",
              "      <td>81.906250</td>\n",
              "      <td>0.024250</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.001758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.863704</td>\n",
              "      <td>0.301940</td>\n",
              "      <td>75.500000</td>\n",
              "      <td>0.025205</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.007454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.672917</td>\n",
              "      <td>0.444663</td>\n",
              "      <td>75.875000</td>\n",
              "      <td>0.032587</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.001042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.815625</td>\n",
              "      <td>0.573136</td>\n",
              "      <td>79.093750</td>\n",
              "      <td>0.021982</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.304088</td>\n",
              "      <td>77.125000</td>\n",
              "      <td>0.031765</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.653223</td>\n",
              "      <td>0.416559</td>\n",
              "      <td>77.312500</td>\n",
              "      <td>0.022574</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.003223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.678125</td>\n",
              "      <td>0.456907</td>\n",
              "      <td>80.406250</td>\n",
              "      <td>0.020678</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.643750</td>\n",
              "      <td>0.518270</td>\n",
              "      <td>85.531250</td>\n",
              "      <td>0.021515</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.529884</td>\n",
              "      <td>99.625000</td>\n",
              "      <td>0.021718</td>\n",
              "      <td>0.278125</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.601172</td>\n",
              "      <td>0.576450</td>\n",
              "      <td>77.312500</td>\n",
              "      <td>0.025261</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.181250</td>\n",
              "      <td>0.004297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.571094</td>\n",
              "      <td>0.550153</td>\n",
              "      <td>82.562500</td>\n",
              "      <td>0.016841</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.002344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.693750</td>\n",
              "      <td>0.503127</td>\n",
              "      <td>87.343750</td>\n",
              "      <td>0.022860</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.782454</td>\n",
              "      <td>0.512799</td>\n",
              "      <td>72.812500</td>\n",
              "      <td>0.029517</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.007454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.880697</td>\n",
              "      <td>0.437872</td>\n",
              "      <td>80.750000</td>\n",
              "      <td>0.021765</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.002572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.465625</td>\n",
              "      <td>0.520134</td>\n",
              "      <td>87.750000</td>\n",
              "      <td>0.023423</td>\n",
              "      <td>0.278125</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.721582</td>\n",
              "      <td>0.515247</td>\n",
              "      <td>82.593750</td>\n",
              "      <td>0.021024</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.002832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.717676</td>\n",
              "      <td>0.393825</td>\n",
              "      <td>75.125000</td>\n",
              "      <td>0.028866</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.005176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.723112</td>\n",
              "      <td>0.535696</td>\n",
              "      <td>71.156250</td>\n",
              "      <td>0.030195</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.010612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.892481</td>\n",
              "      <td>0.450936</td>\n",
              "      <td>67.906250</td>\n",
              "      <td>0.032274</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.008105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.596061</td>\n",
              "      <td>0.544691</td>\n",
              "      <td>97.781250</td>\n",
              "      <td>0.022000</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.005436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.893783</td>\n",
              "      <td>0.504876</td>\n",
              "      <td>73.687500</td>\n",
              "      <td>0.027079</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.003158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.747982</td>\n",
              "      <td>0.554625</td>\n",
              "      <td>74.343750</td>\n",
              "      <td>0.046495</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.007357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.824349</td>\n",
              "      <td>0.546497</td>\n",
              "      <td>68.625000</td>\n",
              "      <td>0.026592</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.008724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.758203</td>\n",
              "      <td>0.409077</td>\n",
              "      <td>68.843750</td>\n",
              "      <td>0.029391</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.011328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.652995</td>\n",
              "      <td>0.377879</td>\n",
              "      <td>68.250000</td>\n",
              "      <td>0.027124</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.002995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.615625</td>\n",
              "      <td>0.444407</td>\n",
              "      <td>75.125000</td>\n",
              "      <td>0.028283</td>\n",
              "      <td>0.415625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.821875</td>\n",
              "      <td>0.423136</td>\n",
              "      <td>67.937500</td>\n",
              "      <td>0.030654</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.931869</td>\n",
              "      <td>0.445122</td>\n",
              "      <td>76.406250</td>\n",
              "      <td>0.029376</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.006868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.785287</td>\n",
              "      <td>0.516674</td>\n",
              "      <td>77.875000</td>\n",
              "      <td>0.025699</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.004036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.759961</td>\n",
              "      <td>0.507500</td>\n",
              "      <td>71.875000</td>\n",
              "      <td>0.028348</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.006836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.687054</td>\n",
              "      <td>0.325067</td>\n",
              "      <td>77.535714</td>\n",
              "      <td>0.018893</td>\n",
              "      <td>0.489286</td>\n",
              "      <td>0.192857</td>\n",
              "      <td>0.004911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.716048</td>\n",
              "      <td>0.314874</td>\n",
              "      <td>77.312500</td>\n",
              "      <td>0.025561</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.003548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.687760</td>\n",
              "      <td>0.506039</td>\n",
              "      <td>78.843750</td>\n",
              "      <td>0.028666</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.003385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.710221</td>\n",
              "      <td>0.530922</td>\n",
              "      <td>78.781250</td>\n",
              "      <td>0.043114</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.003971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.959668</td>\n",
              "      <td>0.231836</td>\n",
              "      <td>73.281250</td>\n",
              "      <td>0.045972</td>\n",
              "      <td>0.759375</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.006543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.785775</td>\n",
              "      <td>0.535063</td>\n",
              "      <td>83.593750</td>\n",
              "      <td>0.024130</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.004525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.753125</td>\n",
              "      <td>0.285636</td>\n",
              "      <td>66.062500</td>\n",
              "      <td>0.031974</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.684375</td>\n",
              "      <td>0.502521</td>\n",
              "      <td>72.281250</td>\n",
              "      <td>0.026244</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.825749</td>\n",
              "      <td>0.427797</td>\n",
              "      <td>68.656250</td>\n",
              "      <td>0.033505</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.010124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.854199</td>\n",
              "      <td>0.415353</td>\n",
              "      <td>77.250000</td>\n",
              "      <td>0.025238</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.004199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.893587</td>\n",
              "      <td>0.464365</td>\n",
              "      <td>64.562500</td>\n",
              "      <td>0.043244</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.009212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.907520</td>\n",
              "      <td>0.307802</td>\n",
              "      <td>72.437500</td>\n",
              "      <td>0.036517</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.016895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.836947</td>\n",
              "      <td>0.446863</td>\n",
              "      <td>72.718750</td>\n",
              "      <td>0.031807</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.015072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.934310</td>\n",
              "      <td>0.331774</td>\n",
              "      <td>74.781250</td>\n",
              "      <td>0.028138</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.015560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.656152</td>\n",
              "      <td>0.541838</td>\n",
              "      <td>62.437500</td>\n",
              "      <td>0.030797</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.006152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.861263</td>\n",
              "      <td>0.516468</td>\n",
              "      <td>80.500000</td>\n",
              "      <td>0.039620</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.011263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.757650</td>\n",
              "      <td>0.545118</td>\n",
              "      <td>68.343750</td>\n",
              "      <td>0.030575</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.010775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.908789</td>\n",
              "      <td>0.464968</td>\n",
              "      <td>74.250000</td>\n",
              "      <td>0.036372</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.024414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.853158</td>\n",
              "      <td>0.433588</td>\n",
              "      <td>76.062500</td>\n",
              "      <td>0.045219</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.003158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.751074</td>\n",
              "      <td>0.540490</td>\n",
              "      <td>66.937500</td>\n",
              "      <td>0.034919</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.004199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.821289</td>\n",
              "      <td>0.445700</td>\n",
              "      <td>73.031250</td>\n",
              "      <td>0.027288</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.011914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.818294</td>\n",
              "      <td>0.425374</td>\n",
              "      <td>74.875000</td>\n",
              "      <td>0.030970</td>\n",
              "      <td>0.621875</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.002669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.753125</td>\n",
              "      <td>0.502521</td>\n",
              "      <td>65.343750</td>\n",
              "      <td>0.028792</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.895085</td>\n",
              "      <td>0.432055</td>\n",
              "      <td>61.906250</td>\n",
              "      <td>0.029044</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.004460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.967481</td>\n",
              "      <td>0.487493</td>\n",
              "      <td>65.812500</td>\n",
              "      <td>0.037495</td>\n",
              "      <td>0.759375</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.008105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>1.032227</td>\n",
              "      <td>0.212884</td>\n",
              "      <td>72.750000</td>\n",
              "      <td>0.033372</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.004102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.751139</td>\n",
              "      <td>0.439573</td>\n",
              "      <td>67.500000</td>\n",
              "      <td>0.030109</td>\n",
              "      <td>0.553125</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.004264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>1.007422</td>\n",
              "      <td>0.380414</td>\n",
              "      <td>64.562500</td>\n",
              "      <td>0.035794</td>\n",
              "      <td>0.793750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.019922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.684831</td>\n",
              "      <td>0.614168</td>\n",
              "      <td>61.906250</td>\n",
              "      <td>0.040589</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.012956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.973958</td>\n",
              "      <td>0.300208</td>\n",
              "      <td>81.156250</td>\n",
              "      <td>0.030228</td>\n",
              "      <td>0.759375</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.014583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.789779</td>\n",
              "      <td>0.437434</td>\n",
              "      <td>64.406250</td>\n",
              "      <td>0.034964</td>\n",
              "      <td>0.587500</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.002279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.674447</td>\n",
              "      <td>0.525652</td>\n",
              "      <td>68.562500</td>\n",
              "      <td>0.042803</td>\n",
              "      <td>0.484375</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.002572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.952376</td>\n",
              "      <td>0.314251</td>\n",
              "      <td>66.406250</td>\n",
              "      <td>0.037649</td>\n",
              "      <td>0.759375</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.005501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.931771</td>\n",
              "      <td>0.372666</td>\n",
              "      <td>68.093750</td>\n",
              "      <td>0.032628</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.013021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.723600</td>\n",
              "      <td>0.551622</td>\n",
              "      <td>69.062500</td>\n",
              "      <td>0.034055</td>\n",
              "      <td>0.518750</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.011100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.863444</td>\n",
              "      <td>0.441715</td>\n",
              "      <td>71.750000</td>\n",
              "      <td>0.037815</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.007194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.926823</td>\n",
              "      <td>0.452592</td>\n",
              "      <td>64.406250</td>\n",
              "      <td>0.028281</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.008073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>0.974023</td>\n",
              "      <td>0.297746</td>\n",
              "      <td>65.906250</td>\n",
              "      <td>0.060780</td>\n",
              "      <td>0.759375</td>\n",
              "      <td>0.193750</td>\n",
              "      <td>0.020898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.932031</td>\n",
              "      <td>0.401254</td>\n",
              "      <td>65.687500</td>\n",
              "      <td>0.039664</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.019531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.955111</td>\n",
              "      <td>0.308900</td>\n",
              "      <td>58.687500</td>\n",
              "      <td>0.042649</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.030111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.900163</td>\n",
              "      <td>0.354476</td>\n",
              "      <td>72.250000</td>\n",
              "      <td>0.037408</td>\n",
              "      <td>0.690625</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.009538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[Sample A]:\n",
            "Learning Prepositional Paraphrases for Noun Compounds through LSTM-Based Sequence Modeling\n",
            "\n",
            "[Sample B]:\n",
            "Treat us like the sequences we are: Prepositional Paraphrasing of Noun Compounds using LSTM\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is clever and plays with the metaphor of treating sequences like themselves. It seems to highlight the self-referential nature of the approach, which is intriguing and engaging. Sample A, while clear and straightforward, lacks this playfulness.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 590) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Pricing is essential in optimizing transportation resource allocation. Congestion pricing is widely used to reduce urban traffic congestion. We propose and investigate a novel Dynamic Pricing Strategy (DPS) to price travelers’ trips in intelligent transportation platforms (e.g., DiDi, Lyft, Uber). The trips are charged according to their “congestion contributions” to global urban traffic systems. The dynamic pricing strategy retrieves a matching between n travelers’ trips and the potential travel routes (each trip has k potential routes) to minimize the global traffic congestion. We believe that DPS holds the potential to benefit society and the environment, such as reducing traffic congestion and enabling smarter and greener transportation. The DPS problem is challenging due to its high computation complexity (there exist k matching possibilities). We develop an efficient and effective approximate matching algorithm based on local search, as well as pruning techniques to further enhance the matching efficiency. The accuracy and efficiency of the dynamic pricing strategy are verified by extensive experiments on real datasets.\n",
            "\n",
            "[Sample A]:\n",
            "A Dynamic Pricing Strategy for Congestion-Aware Trip Allocation in Urban Transportation Networks\n",
            "\n",
            "[Sample B]:\n",
            "Pay Your Trip for Traffic Congestion: Dynamic Pricing in Traffic-Aware Road Networks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and directly conveys the focus on dynamic pricing and congestion-aware trip allocation, which are the main topics. Sample B introduces the concept of traffic congestion more explicitly but lacks the focus on urban transportation networks.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 595) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Motivated by work predicting coarsegrained author categories in social media, such as gender or political preference, we explore whether Twitter contains information to support the prediction of finegrained categories, or social roles. We find that the simple self-identification pattern “I am a ” supports significantly richer classification than previously explored, successfully retrieving a variety of fine-grained roles. For a given role (e.g., writer), we can further identify characteristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role.\n",
            "\n",
            "[Sample A]:\n",
            "Analyzing Twitter User Self-Identification Phrases for Predicting Fine-Grained Social Roles and Attributes\n",
            "\n",
            "[Sample B]:\n",
            "I'm a Belieber: Social Roles via Self-identification and Conceptual Attributes\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B creates a catchy tagline that emphasizes the connection between self-identification and social roles, suggesting an emotional or personal narrative. It also hints at the intriguing concept of connecting Twitter users to their roles and attributes through simple phrases. Sample A, while informative, feels more academic and lacks the intrigue and punchiness of Sample B.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 600) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.\n",
            "\n",
            "[Sample A]:\n",
            "A Diagnostic Method for Evaluating Cross-Modal Interactions in Multimodal Machine Learning Models Using Empirical Multimodally-Additive Function Projection\n",
            "\n",
            "[Sample B]:\n",
            "Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The title in Sample A emphasizes the diagnostic tool and its purpose, which can be funnier as it directly relates to the core of the research. However, Sample B is more direct and to the point, which might be perceived as slightly more serious or less playful.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 605) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Despite the recent advances in coherence modelling, most such models including state-of-the-art neural ones, are evaluated on either contrived proxy tasks such as the standard order discrimination benchmark, or tasks that require special expert annotation. Moreover, most evaluations are conducted on small newswire corpora. To address these shortcomings, in this paper we propose four generic evaluation tasks that draw on different aspects of coherence at both the lexical and document levels, and can be applied to any corpora. In designing these tasks, we aim at capturing coherence-specific properties, such as the correct use of discourse connectives, lexical cohesion, as well as the overall temporal and causal consistency among events and participants in a story. Importantly, our proposed tasks either rely on automatically-generated data, or data annotated for other purposes, hence alleviating the need for annotation specifically targeted to the task of coherence modelling. We perform experiments with several existing state-of-the-art neural models of coherence on these tasks, across large corpora from different domains, including newswire, dialogue, as well as narrative and instructional text. Our findings point to a strong need for revisiting the common practices in the development and evaluation of coherence models.\n",
            "\n",
            "[Sample A]:\n",
            "Evaluation of Neural Coherence Models Across Multiple Domains Using Generic Coherence-Specific Tasks\n",
            "\n",
            "[Sample B]:\n",
            "How coherent are neural models of coherence?\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are similar in their focus on the evaluation of neural coherence models, but Sample A is more formal and academic. Sample B is more conversational and asks a direct question. Sample A emphasizes the use of generic coherence-specific tasks, while Sample B doesn't mention these tasks specifically.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 610) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a very simple, and yet counter-intuitive, postprocessing technique – eliminate the common mean vector and a few top dominating directions from the word vectors – that renders off-the-shelf representations even stronger. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and text classification) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.\n",
            "\n",
            "[Sample A]:\n",
            "All-but-the-Top: Simple and Effective Postprocessing for Word Representations\n",
            "\n",
            "[Sample B]:\n",
            "An Empirical Study on Improving Word Embeddings by Removing Common Components Across Multiple NLP Tasks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and impactful. It directly states the main idea of the postprocessing technique without unnecessary elaboration.\n",
            "Sample B introduces a broader theme of improving word embeddings but lacks clarity on the specific method being discussed.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 615) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.\n",
            "\n",
            "[Sample A]:\n",
            "Examples are not enough, learn to criticize! Criticism for Interpretability\n",
            "\n",
            "[Sample B]:\n",
            "A Method for Selecting Prototypes and Criticisms to Improve Interpretability of Complex Data Distributions\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The first title, \"Examples are not enough, learn to criticize!\", is more concise and evocative. It captures the essence of the paper's methodology and goals succinctly. The second title, \"A Method for Selecting Prototypes and Criticisms to Improve Interpretability of Complex Data Distributions\", provides some information but fails to convey the core message immediately.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 620) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known “experts” setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds.\n",
            "\n",
            "[Sample A]:\n",
            "Analysis of Adversarial Online Learning with Partial and Side Observations Using Graph-Based Feedback Structures\n",
            "\n",
            "[Sample B]:\n",
            "From Bandits to Experts: On the Value of Side-Observations\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A emphasizes the unique graph feedback structure and its implications on adversarial online learning, capturing its core ideas succinctly. Sample B focuses more on the concept of side observations in relation to multi-armed bandits, which might not as clearly address the core of the research.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 625) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora. In this role they play an important part in making those resources usable for a wider audience. Over the past decades, several such query systems and languages have emerged, varying greatly in their expressiveness and technical details. This paper offers a broad overview of the history of corpora and corpus query tools. It focusses strongly on the query side and hints at exciting directions for future development.\n",
            "\n",
            "[Sample A]:\n",
            "A Survey of Corpus Query Systems and Their Development\n",
            "\n",
            "[Sample B]:\n",
            "To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and intriguing with its use of \"boldly querying\" which sounds adventurous and forward-thinking. \"Corpus Query Systems and Their Development\" is more formal and less dramatic.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 630) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Comparisons are common linguistic devices used to indicate the likeness of two things. Often, this likeness is not meant in the literal sense—for example, “I slept like a log” does not imply that logs actually sleep. In this paper we propose a computational study of figurative comparisons, or similes. Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness. We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon. We operationalize these insights and apply them to a new task with high relevance to text understanding: distinguishing between figurative and literal comparisons. Finally, we apply this framework to explore the social context in which figurative language is produced, showing that similes are more likely to accompany opinions showing extreme sentiment, and that they are uncommon in reviews deemed helpful.\n",
            "\n",
            "[Sample A]:\n",
            "A Computational Analysis of Figurative and Literal Comparisons in User-Generated Product Reviews\n",
            "\n",
            "[Sample B]:\n",
            "Brighter than Gold: Figurative Language in User Generated Comparisons\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A directly summarizes the main point of the paper, which is the computational analysis of figurative and literal comparisons in user-generated product reviews. This title is direct and informative, effectively conveying the purpose of the research.\n",
            "\n",
            "Sample B uses a metaphor comparing figurative language to gold, which is an attempt to be more memorable and attention-grabbing. However, it risks the potential of confusing the reader as to the true nature of the research.\n",
            "\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 635) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Cross-lingual word embeddings (CLWE) underlie many multilingual natural language processing systems, often through orthogonal transformations of pre-trained monolingual embeddings. However, orthogonal mapping only works on language pairs whose embeddings are naturally isomorphic. For non-isomorphic pairs, our method (Iterative Normalization) transforms monolingual embeddings to make orthogonal alignment easier by simultaneously enforcing that (1) individual word vectors are unit length, and (2) each language's average vector is zero. Iterative Normalization consistently improves word translation accuracy of three CLWE methods, with the largest improvement observed on English-Japanese (from 2% to 44% test accuracy).\n",
            "\n",
            "[Sample A]:\n",
            "Are Girls Neko or Shōjo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization\n",
            "\n",
            "[Sample B]:\n",
            "Improving Cross-Lingual Word Embedding Alignment for Non-Isomorphic Language Pairs Using Iterative Normalization\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B focuses on the technique and its improvement, aligning it more closely with the scientific rigor of research, while Sample A introduces a new context and a question that is likely to engage the reader's curiosity more effectively. The abstract about the improvement in accuracy emphasizes a more immediate, tangible outcome.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 640) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data. We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction; integrating sentiment analysis into KPA; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata. We show empirically that these novel extensions of KPA substantially improve its performance. We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement.\n",
            "\n",
            "[Sample A]:\n",
            "Extending Key Point Analysis for Improved Summarization of Business Reviews with Sentiment Integration and Collective Key Point Mining\n",
            "\n",
            "[Sample B]:\n",
            "Every Bite Is an Experience: Key Point Analysis of Business Reviews\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A emphasizes the innovative aspect of KPA's improvements, focusing on better summarization and incorporating sentiment analysis. It also notes the significant performance improvement. Sample B focuses solely on the topic of business reviews and lacks the explanatory and integrative details.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 645) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Understanding common entities and their attributes is a primary requirement for any system that comprehends natural language. In order to enable learning about common entities, we introduce a novel machine comprehension task, GuessTwo: given a short paragraph comparing different aspects of two real-world semantically-similar entities, a system should guess what those entities are. Accomplishing this task requires deep language understanding which enables inference, connecting each comparison paragraph to different levels of knowledge about world entities and their attributes. So far we have crowdsourced a dataset of more than 14K comparison paragraphs comparing entities from a variety of categories such as fruits and animals. We have designed two schemes for evaluation: open-ended, and binary-choice prediction. For benchmarking further progress in the task, we have collected a set of paragraphs as the test set on which human can accomplish the task with an accuracy of 94.2% on open-ended prediction. We have implemented various models for tackling the task, ranging from semantic-driven to neural models. The semantic-driven approach outperforms the neural models, however, the results indicate that the task is very challenging across the models.\n",
            "\n",
            "[Sample A]:\n",
            "Apples to Apples: Learning Semantics of Common Entities Through a Novel Comprehension Task\n",
            "\n",
            "[Sample B]:\n",
            "A Machine Comprehension Task for Identifying Semantically Similar Entity Pairs Using Comparative Paragraphs\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A focuses on apples-to-apples and entity semantics, making it more engaging and relatable. The playful title encourages users to explore the concept further. Sample B, on the other hand, is more neutral and straightforward, which might not grab as much attention. Sample A is also more specific, highlighting the application of the task in the field of natural language understanding.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 650) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research. We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines.\n",
            "\n",
            "[Sample A]:\n",
            "Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?\n",
            "\n",
            "[Sample B]:\n",
            "Ethical Considerations and Decision-Making Processes for Appropriate Uses of NLP Research, with a Focus on Automatic Legal Sentencing\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The first title, \"Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?\" is more intriguing and thought-provoking. It uses dark humor and a provocative question that grabs the reader's attention and evokes a strong emotional response.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 655) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The advent of social media has brought Internet memes, a unique social phenomenon, to the front stage of the Web. Embodied in the form of images with text descriptions, little do we know about the “language of memes”. In this paper, we statistically study the correlations among popular memes and their wordings, and generate meme descriptions from raw images. To do this, we take a multimodal approach—we propose a robust nonparanormal model to learn the stochastic dependencies among the image, the candidate descriptions, and the popular votes. In experiments, we show that combining text and vision helps identifying popular meme descriptions; that our nonparanormal model is able to learn dense and continuous vision features jointly with sparse and discrete text features in a principled manner, outperforming various competitive baselines; that our system can generate meme descriptions using a simple pipeline.\n",
            "\n",
            "[Sample A]:\n",
            "A Multimodal Statistical Framework for Predicting and Generating Internet Meme Descriptions Using Textual and Visual Features\n",
            "\n",
            "[Sample B]:\n",
            "I Can Has Cheezburger? A Nonparanormal Approach to Combining Textual and Visual Information for Predicting and Generating Popular Meme Descriptions\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are clear and informative, but Sample B is more engaging and fun. The title \"I Can Has Cheezburger?\" conveys a sense of humor and relatability, which can draw readers in. The tone is colloquial and playful, encouraging a deeper curiosity in the reader.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 660) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1\n",
            "\n",
            "[Sample A]:\n",
            "A Method for Mapping High-Level Goal-Oriented Instructions to Command Sequences Using an Environment Model and Reinforcement Learning\n",
            "\n",
            "[Sample B]:\n",
            "Reading between the Lines: Learning to Map High-Level Instructions to Commands\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is clear and concise, emphasizing the method's innovation with specific terms. Sample B lacks the technical terminology and implies more of a cognitive or literary approach rather than a computational one.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 665) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Automatic question generation (QG) is a useful yet challenging task in NLP. Recent neural network-based approaches represent the state-of-the-art in this task. In this work, we attempt to strengthen them significantly by adopting a holistic and novel generator-evaluator framework that directly optimizes objectives that reward semantics and structure. The generator is a sequence-to-sequence model that incorporates the structure and semantics of the question being generated. The generator predicts an answer in the passage that the question can pivot on. Employing the copy and coverage mechanisms, it also acknowledges other contextually important (and possibly rare) keywords in the passage that the question needs to conform to, while not redundantly repeating words. The evaluator model evaluates and assigns a reward to each predicted question based on its conformity to the structure of ground-truth questions. We propose two novel QG-specific reward functions for text conformity and answer conformity of the generated question. The evaluator also employs structure-sensitive rewards based on evaluation measures such as BLEU, GLEU, and ROUGE-L, which are suitable for QG. In contrast, most of the previous works only optimize the cross-entropy loss, which can induce inconsistencies between training (objective) and testing (evaluation) measures. Our evaluation shows that our approach significantly outperforms state-of-the-art systems on the widely-used SQuAD benchmark as per both automatic and human evaluation.\n",
            "\n",
            "[Sample A]:\n",
            "Putting the Horse before the Cart: A Generator-Evaluator Framework for Question Generation from Text\n",
            "\n",
            "[Sample B]:\n",
            "A Generator-Evaluator Architecture Optimizing Structure and Semantic Objectives for Neural Question Generation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The abstract seems to be discussing advances in neural question generation (QG) from text, considering both structure and semantics. The text highlights the use of a generator-evaluator framework aimed at optimizing QG tasks via multiple rewards. The proposed QG-specific reward functions for text conformity and answer conformity are also explained. The evaluation part shows superior performance compared to existing state-of-the-art QG systems using a variety of measures. The abstract avoids humor and does not employ humor-oriented language. \n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 670) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We describe a synchronous parsing algorithm that is based on two successive monolingual parses of an input sentence pair. Although the worst-case complexity of this algorithm is and must be O(n) for binary SCFGs, its average-case run-time is far better. We demonstrate that for a number of common synchronous parsing problems, the two-parse algorithm substantially outperforms alternative synchronous parsing strategies, making it efficient enough to be utilized without resorting to a pruned search.\n",
            "\n",
            "[Sample A]:\n",
            "Two monolingual parses are better than one (synchronous parse)\n",
            "\n",
            "[Sample B]:\n",
            "An Efficient Synchronous Parsing Algorithm Based on Successive Monolingual Parses\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A: \"Two monolingual parses are better than one (synchronous parse)\"\n",
            "Sample B: \"An Efficient Synchronous Parsing Algorithm Based on Successive Monolingual Parses\"\n",
            "The title \"Two monolingual parses are better than one (synchronous parse)\" is more captivating as it directly addresses the core of the research, highlighting that two monolingual parses are superior to one. This phrase captures the essence of the research, emphasizing the efficiency and effectiveness of the method, which resonates with the audience and draws them in.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 675) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This work addresses the problem of regret minimization in non-stochastic multiarmed bandit problems, focusing on performance guarantees that hold with high probability. Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation. One of these modifications is forcing the learner to sample arms from the uniform distribution at least Ω( √ T ) times over T rounds, which can adversely affect performance if many of the arms are suboptimal. While it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component. Our result relies on a simple and intuitive loss-estimation strategy called Implicit eXploration (IX) that allows a remarkably clean analysis. To demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework. Finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique.\n",
            "\n",
            "[Sample A]:\n",
            "Explore no more: Improved high-probability regret bounds for non-stochastic bandits\n",
            "\n",
            "[Sample B]:\n",
            "High-Probability Regret Bounds in Non-Stochastic Multi-Armed Bandits Using Implicit Exploration Methods\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more humorous because it uses the phrase \"Explore no more,\" which is typically associated with a boring or uninteresting activity. This phrasing has an immediate sense of excitement and a playful tone that matches the abstract's content of improving on existing work and providing innovative insights.\n",
            "</notes>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 680) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Text effects transfer technology automatically makes the text dramatically more impressive. However, previous style transfer methods either study the model for general style, which cannot handle the highly-structured text effects along the glyph, or require manual design of subtle matching criteria for text effects. In this paper, we focus on the use of the powerful representation abilities of deep neural features for text effects transfer. For this purpose, we propose a novel Texture Effects Transfer GAN (TET-GAN), which consists of a stylization subnetwork and a destylization subnetwork. The key idea is to train our network to accomplish both the objective of style transfer and style removal, so that it can learn to disentangle and recombine the content and style features of text effects images. To support the training of our network, we propose a new text effects dataset with as much as 64 professionally designed styles on 837 characters. We show that the disentangled feature representations enable us to transfer or remove all these styles on arbitrary glyphs using one network. Furthermore, the flexible network design empowers TET-GAN to efficiently extend to a new text style via oneshot learning where only one example is required. We demonstrate the superiority of the proposed method in generating high-quality stylized text over the state-of-the-art methods.\n",
            "\n",
            "[Sample A]:\n",
            "A Generative Adversarial Network Approach for Disentangled Transfer and Removal of Text Effects using Deep Neural Features\n",
            "\n",
            "[Sample B]:\n",
            "TET-GAN: Text Effects Transfer via Stylization and Destylization\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more attention-grabbing but sample B is more descriptive. Both are okay but TET-GAN is specific to the text effects transfer scenario.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 685) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more seriously due to the utilization of more training data. We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model's generalization ability, but also assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect.\n",
            "\n",
            "[Sample A]:\n",
            "Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models\n",
            "\n",
            "[Sample B]:\n",
            "Mitigating Semantic Similarity Bias in Pretrained Language Models for the COPA Task Through Regularization Techniques\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B more clearly highlights the specific task and the solution to mitigate semantic bias, making it more to the point and thus potentially more funny as it seems to make light of the inherent flaws of a specific research technique rather than questioning its effectiveness or ethical implications.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 690) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Manga is a world popular comic form originated in Japan, which typically employs black-and-white stroke lines and geometric exaggeration to describe humans’ appearances, poses, and actions. In this paper, we propose MangaGAN, the first method based on Generative Adversarial Network (GAN) for unpaired photo-to-manga translation. Inspired by the drawing process of experienced manga artists, MangaGAN generates geometric features and converts each facial region into the manga domain with a tailored multi-GANs architecture. For training MangaGAN, we collect a new dataset from a popular manga work with extensive features. To produce high-quality manga faces, we propose a structural smoothing loss to smooth stroke-lines and avoid noisy pixels, and a similarity preserving module to improve the similarity between domains of photo and manga. Extensive experiments show that MangaGAN can produce high-quality manga faces preserving both the facial similarity and manga style, and outperforms other reference methods.\n",
            "\n",
            "[Sample A]:\n",
            "MangaGAN: Unpaired Photo-to-Manga Translation Based on The Methodology of Manga Drawing\n",
            "\n",
            "[Sample B]:\n",
            "A Generative Adversarial Network Approach for Translating Unpaired Photographic Images into Manga Style Using Geometric Feature Generation and Domain Similarity Preservation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are informative and convey the essence of the research, but Sample A might be perceived as slightly more concise and punchy, suggesting a creative or humorous twist on the conventional title. The abstract doesn't explicitly mention a \"Generative Adversarial Network approach,\" which might catch the reader off guard or make Sample B seem more specific and detailed. However, Sample B is more precise in its description of the method, which could be a plus for researchers familiar with GANs and image-to-image translation.\n",
            "\n",
            "Overall, Sample A is slightly more engaging and could potentially appeal to a broader audience, making it marginally more Funnier.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 695) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.\n",
            "\n",
            "[Sample A]:\n",
            "Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation\n",
            "\n",
            "[Sample B]:\n",
            "An Active Learning Approach to Mitigating Diminishing Returns in Large-Scale Statistical Machine Translation Data Acquisition\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "These titles are both informative but Sample A is more concise and has a stronger call to action. Sample B is more metaphorical and interesting, which might appeal to readers more in terms of engaging language. Without comparing both titles directly, Sample A seems superior in terms of punch and engagement.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 700) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This paper introduces Logical Semantics with Perception (LSP), a model for grounded language acquisition that learns to map natural language statements to their referents in a physical environment. For example, given an image, LSP can map the statement \"blue mug on the table\" to the set of image segments showing blue mugs on tables. LSP learns physical representations for both categorical (\"blue,\" \"mug\") and relational (\"on\") language, and also learns to compose these representations to produce the referents of entire statements. We further introduce a weakly supervised training procedure that estimates LSP's parameters using annotated referents for entire statements, without annotated referents for individual words or the parse structure of the statement. We perform experiments on two applications: scene understanding and geographical question answering. We find that LSP outperforms existing, less expressive models that cannot represent relational language. We further find that weakly supervised training is competitive with fully supervised training while requiring significantly less annotation effort.\n",
            "\n",
            "[Sample A]:\n",
            "Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World\n",
            "\n",
            "[Sample B]:\n",
            "A Weakly Supervised Model for Mapping Natural Language Statements to Physical Environment Segments Using Logical Semantics and Perception\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more clear and straightforward, highlighting the joint learning of parsing and perception. It also implies a stronger connection to the current research, which could be appealing to a wider audience. Sample B, on the other hand, introduces a weakly supervised model for mapping statements to environment segments, which may not be immediately clear what problem it solves or how it differs from existing work.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 705) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We introduce a new set of benchmark datasets derived from ACLED data for fine-grained event classification and compare the performance of various state-of-the-art models on these datasets, including SVM based on TF-IDF character n-grams and neural context-free embeddings (GLOVE and FASTTEXT) as well as deep learning-based BERT with its contextual embeddings. The best results in terms of micro (94.3-94.9%) and macro F1 (86.0-88.9%) were obtained using BERT transformer, with simpler TF-IDF character n-gram based SVM being an interesting alternative. Further, we discuss the pros and cons of the considered benchmark models in terms of their robustness and the dependence of the classification performance on the size of training data.\n",
            "\n",
            "[Sample A]:\n",
            "New Benchmark Corpus and Models for Fine-grained Event Classification: To BERT or not to BERT?\n",
            "\n",
            "[Sample B]:\n",
            "Benchmark Datasets and Model Comparisons for Fine-grained Event Classification using ACLED Data\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Samples A and B both discuss fine-grained event classification using ACLED data. Neither seems inherently funnier than the other. The titles focus on the same theme and information.\n",
            "Sample B is more concise and directly states the purpose of the research without being overly complex.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 710) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Cyberbullying statistics are shocking, the number of affected young people is increasing dramatically with the affordability of mobile technology devices combined with a growing number of social networks. This paper proposes a framework to analyse Tweets with the goal to identify cyberharassment in social networks as an important step to protect people from cyberbullying. The proposed framework incorporates latent or hidden variables with supervised learning to determine potential bullying cases resembling short blogging type texts such as Tweets. It uses the LIWC2007 - tool that translates Tweet messages into 67 numeric values, representing 67 word categories. The output vectors are then used as features for four different classifiers implemented in Weka. Tests on all four classifiers delivered encouraging predictive capability of Tweet messages. Overall it was found that the use of numeric psychometric values outperformed the same algorithms using both filtered and unfiltered words as features. The best performing algorithms was Random Forest with an F1-value of 0.947 using psychometric features compared to a value of 0.847 for the same algorithm using words as features.\n",
            "\n",
            "[Sample A]:\n",
            "A Framework for Detecting Cyberbullying in Tweets Using Psychometric Features and Supervised Learning Methods\n",
            "\n",
            "[Sample B]:\n",
            "\"How Bullying is this Message?\": A Psychometric Thermometer for Bullying\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "These two titles seem very specific to the topic. However, Sample A is more detailed and explicit in its approach, using terms like \"Framework,\" \"Detecting,\" and \"Psychometric Features.\" This makes it more informative and structured. Sample B, while catchy, lacks detail and may be seen as overly simplistic. Both titles effectively convey the core message of the abstract, but Sample A carries more weight due to its specificity and informative tone.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 715) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.\n",
            "\n",
            "[Sample A]:\n",
            "Methods for Automatic Extractive and Abstractive Caption Generation for News Images Using Probabilistic Annotation\n",
            "\n",
            "[Sample B]:\n",
            "How Many Words Is a Picture Worth? Automatic Caption Generation for News Images\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging. It uses the intriguing question \"How Many Words Is a Picture Worth?\" which captures the essence of the problem and makes the abstract more intriguing.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 720) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This paper describes the CMU Let’s Go!! bus information system, an experimental system designed to study the use of spoken dialogue interfaces by non-native speakers. The differences in performance of the speech recognition and language understanding modules of the system when confronted with native and non-native spontaneous speech are analyzed. Focus is placed on the linguistic mismatch between the user input and the system’s expectations, and on its implications in terms of language modeling and parsing performance. The effect of including non-native data when building the speech recognition and language understanding modules is discussed. In order to close the gap between non-native and native input, a method is proposed to automatically generate confirmation prompts that are both close to the user’s input and covered by the system’s language model and grammar, in order to help the user acquire idiomatic expressions appropriate to the task.\n",
            "\n",
            "[Sample A]:\n",
            "Non-Native Users in the Let's Go!! Spoken Dialogue System: Dealing with Linguistic Mismatch\n",
            "\n",
            "[Sample B]:\n",
            "Analysis of Speech Recognition and Language Understanding for Native and Non-Native Speakers in a Bus Information Spoken Dialogue System\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more focused on the specific context (the CMU Let’s Go!! bus information system), while Sample B provides a general overview of the project (analysis of speech recognition and language understanding for native and non-native speakers in a bus information system). Sample B also has a more neutral tone, which might be considered less engaging.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 725) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The Natural Questions (NQ) benchmark set brings new challenges to Machine Reading Comprehension: the answers are not only at different levels of granularity (long and short), but also of richer types (including no-answer, yes/no, single-span and multi-span). In this paper, we target at this challenge and handle all answer types systematically. In particular, we propose a novel approach called Reflection Net which leverages a two-step training procedure to identify the no-answer and wrong-answer cases. Extensive experiments are conducted to verify the effectiveness of our approach. At the time of paper writing (May. 20, 2020), our approach achieved the top 1 on both long and short answer leaderboard, with F1 scores of 77.2 and 64.1, respectively.\n",
            "\n",
            "[Sample A]:\n",
            "A Two-Step Training Approach for Handling Diverse Answer Types in Natural Questions Benchmark Using Reflection Net\n",
            "\n",
            "[Sample B]:\n",
            "No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A: Emphasizes the need for a method to handle diverse answer types effectively. \n",
            "Sample B: Captures the notion that no-answer is superior to wrong-answer which highlights a key distinction in the NQ benchmark.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 730) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Personalized conversation models (PCMs) generate responses according to speaker preferences. Existing personalized conversation tasks typically require models to extract speaker preferences from user descriptions or their conversation histories, which are scarce for newcomers and inactive users. In this paper, we propose a few-shot personalized conversation task with an auxiliary social network. The task requires models to generate personalized responses for a speaker given a few conversations from the speaker and a social network. Existing methods are mainly designed to incorporate descriptions or conversation histories. Those methods can hardly model speakers with so few conversations or connections between speakers. To better cater for newcomers with few resources, we propose a personalized conversation model (PCM) that learns to adapt to new speakers as well as enabling new speakers to learn from resource-rich speakers. Particularly, based on a meta-learning based PCM, we propose a task aggregator (TA) to collect other speakers’ information from the social network. The TA provides prior knowledge of the new speaker in its meta-learning. Experimental results show our methods outperform all baselines in appropriateness, diversity, and consistency with speakers.\n",
            "\n",
            "[Sample A]:\n",
            "Incorporating Social Network Data into Few-Shot Personalized Conversation Models for New and Low-Resource Users\n",
            "\n",
            "[Sample B]:\n",
            "Learning from My Friends: Few-Shot Personalized Conversation Systems via Social Networks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is a concise and direct title that captures the essence of the research, mentioning social network data as an important aspect of the proposed Personalized Conversation Models (PCMs). The title is also more engaging and punchy than Sample B, which may sound a bit generic without the added context of \"learning from my friends\".\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 735) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We report a series of robust empirical observations, demonstrating that deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries – models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn benchmark datasets. Specifically, when fixing the architecture, we describe synthetic datasets for which this pattern is no longer observed. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results reflect how neural networks discover structure in natural datasets.\n",
            "\n",
            "[Sample A]:\n",
            "Let’s Agree to Agree: Neural Networks Share Classification Order on Real Datasets\n",
            "\n",
            "[Sample B]:\n",
            "Empirical Analysis of the Consistent Learning Order Exhibited by Neural Networks Across Architectures and Datasets\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more concise and to the point. It highlights the key finding that neural networks share the same ordering of examples across different architectures and datasets, which is emphasized in the abstract. Sample A, on the other hand, introduces a different topic of agreement about sharing orders among real datasets. While both titles are somewhat humorous and engaging, Sample B maintains the focus on the core message of consistent learning order, making it more effective for the abstract.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 740) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Pirate syndicates capturing tankers to siphon oil, causing an estimated cost of $5 billion a year, has become a serious security issue for maritime traffic. In response to the threat, coast guards and navies deploy patrol boats to protect international oil trade. However, given the vast area of the sea and the highly time and space dependent behaviors of both players, it remains a significant challenge to find efficient ways to deploy patrol resources. In this paper, we address the research challenges and provide four key contributions. First, we construct a Stackelberg model of the oil-siphoning problem based on incident reports of actual attacks; Second, we propose a compact formulation and a constraint generation algorithm, which tackle the exponentially growth of the defender’s and attacker’s strategy spaces, respectively, to compute efficient strategies of security agencies; Third, to further improve the scalability, we propose an abstraction method, which exploits the intrinsic similarity of defender’s strategy space, to solve extremely large-scale games; Finally, we evaluate our approaches through extensive simulations and a detailed case study with real ship traffic data. The results demonstrate that our approach achieves a dramatic improvement of scalability with modest influence on the solution quality and can scale up to realistic-sized problems.\n",
            "\n",
            "[Sample A]:\n",
            "Catching Captain Jack: Efficient Time and Space Dependent Patrols to Combat Oil-Siphoning in International Waters\n",
            "\n",
            "[Sample B]:\n",
            "Scalable Game-Theoretic Algorithms for Optimizing Maritime Patrol Deployment Against Oil-Siphoning Pirates\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Catching Captain Jack: Efficient Time and Space Dependent Patrols to Combat Oil-Siphoning in International Waters\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 745) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efficiently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classification, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classification label and localized latent paths.\n",
            "\n",
            "[Sample A]:\n",
            "A Weakly-Supervised Structured Learning Method Using Eye Gaze for Spatio-Temporal Action Recognition and Localization in Videos\n",
            "\n",
            "[Sample B]:\n",
            "Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and intriguing due to its playful use of metaphor and alliteration. The title encapsulates the idea of gaze being a critical part of learning and helps evoke imagery of observing or understanding actions. It also encourages readers to think about the context-driven guidance provided by eye gaze in the model.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 750) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective_discourse.\n",
            "\n",
            "[Sample A]:\n",
            "A Dataset and Computational Analysis of Subjective Interpretations in English Conversational Discourse\n",
            "\n",
            "[Sample B]:\n",
            "Did they answer? Subjective acts and intents in conversational discourse\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more fitting for academic presentations, as it is concise and directly addresses the core of the study. Sample B is more engaging and captivating, but may not capture the academic rigor of the research being presented.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 755) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Vision-and-Language Navigation (VLN) requires grounding instructions, such as \"turn right and stop at the door\", to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. \"stop at the door\" might ground into visual objects, while \"turn right\" might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.\n",
            "\n",
            "[Sample A]:\n",
            "An Empirical Analysis of Multi-Modal Grounding Approaches for Vision-and-Language Navigation\n",
            "\n",
            "[Sample B]:\n",
            "Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A introduces a more general perspective that encompasses all multi-modal grounding approaches, making it more inclusive. Sample B specifically targets the use of \"Are You Looking?\" as a method of grounding, which may be seen as overly prescriptive. Sample A is also more concise and avoids the need for a leading question in the title.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 760) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We focus on the following natural question: is it possible to influence the outcome of a voting process through the strategic provision of information to voters who update their beliefs rationally? We investigate whether it is computationally tractable to design a signaling scheme maximizing the probability with which the sender’s preferred candidate is elected. We resort to the model recently introduced by Arieli and Babichenko (2019) (i.e., without inter-agent externalities), and focus on, as illustrative examples, k-voting rules and plurality voting. There is a sharp contrast between the case in which private signals are allowed and the more restrictive setting in which only public signals are allowed. In the former, we show that an optimal signaling scheme can be computed efficiently both under a k-voting rule and plurality voting. In establishing these results, we provide two contributions applicable to general settings beyond voting. Specifically, we extend a well-known result by Dughmi and Xu (2017) to more general settings and prove that, when the sender’s utility function is anonymous, computing an optimal signaling scheme is fixed-parameter tractable in the number of receivers’ actions. In the public signaling case, we show that the sender’s optimal expected return cannot be approximated to within any factor under a k-voting rule. This negative result easily extends to plurality voting and problems where utility functions are anonymous.\n",
            "\n",
            "[Sample A]:\n",
            "Computational Complexity of Designing Optimal Signaling Schemes in Voting without Externalities\n",
            "\n",
            "[Sample B]:\n",
            "Persuading Voters: It's Easy to Whisper, It's Hard to Speak Loud\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The Sample B title is more intriguing and reflects a more personal and relatable tone. It emphasizes the practicality by using direct and straightforward language. The title effectively conveys the topic of persuading voters and hints at the complexities through its dual nature of easiness and difficulty. Sample A, on the other hand, is more abstract and technical, focusing on computational complexity without directly addressing the central theme.\n",
            "\n",
            "The more appealing title is Sample B.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 765) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the handengineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.\n",
            "\n",
            "[Sample A]:\n",
            "DeepMath - Deep Sequence Models for Premise Selection\n",
            "\n",
            "[Sample B]:\n",
            "A Two-Stage Neural Sequence Model Approach for Large-Scale Premise Selection in Automated Theorem Proving\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and informative for readers. The title highlights the focus on large-scale premise selection, which is a common point in academic research. It also emphasizes the neural sequence model approach, making it clear what the paper is about. \n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 770) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.\n",
            "\n",
            "[Sample A]:\n",
            "A Framework to Reduce Noise in Distantly Supervised Relation Extraction Using Contrastive Instance Learning Compared to Multi-Instance Learning\n",
            "\n",
            "[Sample B]:\n",
            "CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is generally more straightforward and professional compared to Sample A. Sample A uses more complex language and includes a comparison element, which might make it seem more humorous or clever. However, the abstract itself is well-written and relevant, so Sample B might be more suitable for a serious research audience.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 775) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In-vehicle dialogue systems often contain more than one application, e.g. a navigation and a telephone application. This means that the user might, for example, interrupt the interaction with the telephone application to ask for directions from the navigation application, and then resume the dialogue with the telephone application. In this paper we present an analysis of interruption and resumption behaviour in human-human in-vehicle dialogues and also propose some implications for resumption strategies in an in-vehicle dialogue system.\n",
            "\n",
            "[Sample A]:\n",
            "An Analysis of Interruption and Resumption Behaviors in Multi-Application In-Vehicle Dialogue Systems\n",
            "\n",
            "[Sample B]:\n",
            "Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is considered more engaging and relevant to the discussion of resumption strategies in an in-vehicle dialogue system by highlighting the current state of the system (\"Now, Where Was I?\"). This relates directly to the abstract's mention of interruption and resumption behaviors. However, Sample A is more succinct and to the point, potentially more universally appealing to a broader audience.</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 780) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This paper presents empirical results that contradict the prevailing opinion that entity extraction is a boring solved problem. In particular, we consider data sets that resemble familiar MUC/ACE data, and report surprisingly poor performance for both commercial and research systems. We then give an error analysis that suggests research challenges for entity extraction that are neither boring nor solved.\n",
            "\n",
            "[Sample A]:\n",
            "An Empirical Evaluation of Entity Extraction Performance and Persistent Research Challenges\n",
            "\n",
            "[Sample B]:\n",
            "Entity Extraction is a Boring Solved Problem-Or is it?\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more punchy and engaging. It immediately draws attention to the irony of the research problem by stating that Entity Extraction is a \"Boring Solved Problem\". This title piques the reader's curiosity and uses humor to convey the main point of the discussion.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 785) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Interpreting the inner workings of neural models is a key step in ensuring the robustness and trustworthiness of the models, but work on neural network interpretability typically faces a trade-off: either the models are too constrained to be very useful, or the solutions found by the models are too complex to interpret. We propose a novel strategy for achieving interpretability that - in our experiments - avoids this trade-off. Our approach builds on the success of using probability as the central quantity, such as for instance within the attention mechanism. In our architecture, DoLFIn (Distributions over Latent Features for Interpretability), we do no determine beforehand what each feature represents, and features go altogether into an unordered set. Each feature has an associated probability ranging from 0 to 1, weighing its importance for further processing. We show that, unlike attention and saliency map approaches, this set-up makes it straight-forward to compute the probability with which an input component supports the decision the neural model makes. To demonstrate the usefulness of the approach, we apply DoLFIn to text classification, and show that DoLFIn not only provides interpretable solutions, but even slightly outperforms the classical CNN and BiLSTM text classifiers on the SST2 and AG-news datasets.\n",
            "\n",
            "[Sample A]:\n",
            "DoLFIn: Distributions over Latent Features for Interpretability\n",
            "\n",
            "[Sample B]:\n",
            "A Probabilistic Method for Interpreting Neural Network Decisions through Distributions over Latent Features Applied to Text Classification\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more concise and uses more concrete examples, making it more engaging. The abstract is also more descriptive and clearly outlines the research problem, approach, and findings.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 790) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "As a key representation model of knowledge, ontology has been widely used in a lot of NLP related tasks, such as semantic parsing, information extraction and text mining etc. In this paper, we study the task of ontology matching, which concentrates on finding semantically related entities between different ontologies that describe the same domain, to solve the semantic heterogeneity problem. Previous works exploit different kinds of descriptions of an entity in ontology directly and separately to find the correspondences without considering the higher level correlations between the descriptions. Besides, the structural information of ontology haven’t been utilized adequately for ontology matching. We propose in this paper an ontology matching approach, named ERSOM, which mainly includes an unsupervised representation learning method based on the deep neural networks to learn the general representation of the entities and an iterative similarity propagation method that takes advantage of more abundant structure information of the ontology to discover more mappings. The experimental results on the datasets from Ontology Alignment Evaluation Initiative (OAEI1) show that ERSOM achieves a competitive performance compared to the state-of-the-art ontology matching systems. The OAEI is an international initiative organizing annual campaigns for evaluating ontology matching systems. All of the ontologies provided by OAEI are described in OWL-DL language, and like most of the other participates our ERSOM also manages the OWL ontology in its current version. OAEI: http://oaei.ontologymatching.org/\n",
            "\n",
            "[Sample A]:\n",
            "An Unsupervised Deep Learning and Structural Information-Based Method for Ontology Matching and Alignment\n",
            "\n",
            "[Sample B]:\n",
            "ERSOM: A Structural Ontology Matching Approach Using Automatically Learned Entity Representation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The first title \"An Unsupervised Deep Learning and Structural Information-Based Method for Ontology Matching and Alignment\" is more descriptive and highlights the main components of the proposed method, such as unsupervised learning and structural information. It also gives a clearer idea of what the methodology entails. The second title \"ERSOM: A Structural Ontology Matching Approach Using Automatically Learned Entity Representation\" is less descriptive and focuses mainly on the approach itself, without providing a clear understanding of the methodology or the specific components of the method. The first title is more engaging and easier to remember, potentially leading to better recall.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 795) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.\n",
            "\n",
            "[Sample A]:\n",
            "A Graph Neural Network Approach for Function-Level Vulnerability Detection Using Program Semantics in Open-Source C Code\n",
            "\n",
            "[Sample B]:\n",
            "Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Devign is a general graph neural network based model that learns comprehensive program semantics by training on manually labeled datasets. It utilizes a novel Conv module to efficiently extract useful features in learned rich node representations. The model demonstrates significant improvements over state-of-the-art methods with higher accuracy and F1 scores. The title emphasizes the application of Devign in open-source C code, highlighting its versatility across different projects.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 800) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Multilingual individuals code switch between languages as a part of a complex communication process. However, most computational studies have examined only one or a handful of contextual factors predictive of switching. Here, we examine Naija-English code switching in a rich contextual environment to understand the social and topical factors eliciting a switch. We introduce a new corpus of 330K articles and accompanying 389K comments labeled for code switching behavior. In modeling whether a comment will switch, we show that topic-driven variation, tribal affiliation, emotional valence, and audience design all play complementary roles in behavior.\n",
            "\n",
            "[Sample A]:\n",
            "Analysis of Sociolinguistic Predictors of Naija-English Code-switching in a Large Corpus of Nigerian Online Interactions\n",
            "\n",
            "[Sample B]:\n",
            "Wetin dey with these comments? Modeling Sociolinguistic Factors Affecting Code-switching Behavior in Nigerian Online Discussions\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Analysis of Sociolinguistic Predictors of Naija-English Code-switching in a Large Corpus of Nigerian Online Interactions is a clear, concise title that directly addresses the research question. It is straightforward and relevant, leaving little room for confusion or misinterpretation.\n",
            "\n",
            "However, the abstract is broad and encompasses a variety of sociolinguistic factors influencing code-switching behavior in Nigerian online interactions, which could be quite detailed and extensive. The sample B title, on the other hand, is more intriguing and engaging, using a phrase like \"Wetin dey with these comments?\" that could grab the reader's attention through a humorous or provocative statement.\n",
            "\n",
            "Considering the abstract's content and the sample B title's ability to engage readers through a humorous or provocative statement, Sample B appears to be more fun or witty.\n",
            "</notes>\n",
            "<jud\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 805) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.\n",
            "\n",
            "[Sample A]:\n",
            "An Empirical Analysis of Linguistic Context Utilization in LSTM Language Models Through Ablation Studies\n",
            "\n",
            "[Sample B]:\n",
            "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B captures the essence better. It uses vivid metaphors (\"Sharp Nearby, Fuzzy Far Away\") to illustrate the key points about context use in models, which seems more engaging and funnier.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 810) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks\n",
            "\n",
            "[Sample A]:\n",
            "Integrating Symbolic Syntactic Parse Trees into Transformer-Based Sentence Encoders to Improve Performance and Interpretability\n",
            "\n",
            "[Sample B]:\n",
            "KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A succinctly captures the essence of integrating symbolic syntactic parse trees into transformer-based models without using overly technical terminology, making it more accessible and thus potentially slightly more \"funny\" in terms of readability and engagement.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 815) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-anderror experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.\n",
            "\n",
            "[Sample A]:\n",
            "Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards\n",
            "\n",
            "[Sample B]:\n",
            "A Method for Combining Demonstration and Trial-and-Error Experience with Sparse Rewards in Vision-Based Meta-Learning\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are equally clear and informative. However, Sample A is slightly more impactful in engaging the reader's attention with the catchy phrase \"Watch, Try, Learn\". Yet, Sample B presents a more comprehensive and direct summary of the study's methodology and its benefits.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 820) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We propose a general, yet simple patch that can be applied to existing regularizationbased continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier’s output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier’s output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and significantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL.\n",
            "\n",
            "[Sample A]:\n",
            "CPR: Classifier-Projection Regularization for Continual Learning\n",
            "\n",
            "[Sample B]:\n",
            "A Regularization Approach Using Classifier Output Entropy Projection to Improve Continual Learning Performance\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and focused, likely appealing to a broader audience. Sample B introduces a more specific and academic slant.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 825) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert has labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as “attribution priors” as well as saliency maps for explainable predictions.\n",
            "\n",
            "[Sample A]:\n",
            "Saliency is a Possible Red Herring When Diagnosing Poor Generalization\n",
            "\n",
            "[Sample B]:\n",
            "An Evaluation of Saliency Maps and Auxiliary Mask Labels for Improving Model Generalization Under Covariate Shift\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The first title is more intriguing and has a touch of wit. \"Red Herring\" is a colloquial phrase that implies an incorrect or misleading focus. The phrase \"Saliency is a Possible Red Herring When Diagnosing Poor Generalization\" immediately captures the essence of the research without overly technical jargon. It's more engaging and could be used in a social media post or academic literature review.\n",
            "\n",
            "The second title is more of a straightforward, balanced statement that describes the problem clearly but lacks the punchline or pun element. It's less appealing as a catchy headline or abstract title.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 830) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Human translators routinely have to translate rare inflections of words - due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habláramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the best performing models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.\n",
            "\n",
            "[Sample A]:\n",
            "Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction\n",
            "\n",
            "[Sample B]:\n",
            "An Evaluation of Morphological Generalization in Bilingual Lexicon Induction Across Rare Inflectional Forms\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "These titles are very similar in their intent. Sample A is more colloquial and engaging, emphasizing the \"long tail\" and the importance of infrequent forms. Sample B is more neutral and academic, focusing on the evaluation aspect. Both titles are interesting, but Sample A is funnier and more compelling.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 835) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers, boosting and SVMs are applied to language processing tasks, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various error correction mechanisms have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it. This problem is especially severe if the base classifier has already\n",
            "\n",
            "[Sample A]:\n",
            "An Empirical Analysis of the Effectiveness of Error Correction Methods in Language Processing Classifiers\n",
            "\n",
            "[Sample B]:\n",
            "Why Nitpicking Works: Evidence for Occam's Razor in Error Correctors\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B seems more engaging and compelling in its use of humor and metaphor. By framing the issue in a humorous way, it makes the reader more likely to pay attention and engage with the abstract. The metaphor of nitpicking working as evidence for Occam's Razor is a clever way to express a complex concept in a more accessible and relatable manner.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 840) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. As the core component of information extraction, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we find that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15% F1.\n",
            "\n",
            "[Sample A]:\n",
            "To Link or Not to Link? A Study on End-to-End Tweet Entity Linking\n",
            "\n",
            "[Sample B]:\n",
            "A Structural SVM Approach for Joint Mention Detection and Entity Disambiguation in Twitter Microblogs\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more catchy and directly addresses the core problem. It implies a more complex issue that the study attempts to solve, which grabs the attention of the reader.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 845) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although our models replicate similar gains as recently developed multimodal-integrated systems achieved, our models learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models' interpretability, and discuss how our findings will benefit future research.\n",
            "\n",
            "[Sample A]:\n",
            "Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation\n",
            "\n",
            "[Sample B]:\n",
            "An Empirical Study Investigating the Contribution of Visual Context and Regularization Effects in Multimodal Machine Translation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more captivating and engaging. It immediately captures the reader's attention with a clever pun. Sample B is more academic and factual.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 850) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We introduce a reinforcement learningbased approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders: from verb-final languages (e.g., German) to verb-medial languages (English). In traditional machine translation, a translator must “wait” for source material to appear before translation begins. We remove this bottleneck by predicting the final verb in advance. We use reinforcement learning to learn when to trust predictions about unseen, future portions of the sentence. We also introduce an evaluation metric to measure expeditiousness and quality. We show that our new translation model outperforms batch and monotone translation strategies.\n",
            "\n",
            "[Sample A]:\n",
            "A Reinforcement Learning Approach for Predictive Simultaneous Machine Translation between Languages with Divergent Word Orders\n",
            "\n",
            "[Sample B]:\n",
            "Don't Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more exciting and engaging, as it hints at the dynamic nature of the approach with \"A Reinforcement Learning Approach,\" while Sample B is concise but lacks the intrigue of introducing a new method in the field.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 855) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the first large-scale, open, communitybased effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community.\n",
            "\n",
            "[Sample A]:\n",
            "The Manually Annotated Sub-Corpus: A Community Resource for and by the People\n",
            "\n",
            "[Sample B]:\n",
            "Description of the MASC Project: An Open, Multi-Genre Annotated Subset of the American National Corpus for Community Use\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "MASC's focus on community engagement, inclusivity, and open-access data are highlighted.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 860) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.\n",
            "\n",
            "[Sample A]:\n",
            "An Empirical Analysis of Zero-Shot and Few-Shot Language Transfer Limitations Using Massively Multilingual Transformers\n",
            "\n",
            "[Sample B]:\n",
            "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more straightforward and technical, aiming to provide a detailed empirical analysis of limitations with MMTs.\n",
            "Sample B is more engaging and catchy, promising a journey from zero to hero, which may be more appealing to the reader.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 865) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.\n",
            "\n",
            "[Sample A]:\n",
            "Challenges in Using Word Embedding Analogies for Detecting and Understanding Biases\n",
            "\n",
            "[Sample B]:\n",
            "Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A highlights the complexities and limitations of using word embedding analogies for bias detection, addressing various implementation and subjective choices. It discusses how analogies may be misleading or have exacerbated biases, suggesting the need for a nuanced understanding and correction. Sample B, on the other hand, simplifies the topic to a more relatable and somewhat sensational point about the word \"doctor,\" which, although concise, does not deeply explore the broader implications or challenges associated with the use of analogies for bias detection.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 870) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.\n",
            "\n",
            "[Sample A]:\n",
            "A Laplacian Structured Sparsity Approach for Analyzing Brand-Related Language in Customer Reviews\n",
            "\n",
            "[Sample B]:\n",
            "This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and directly conveys the main point, whereas Sample B uses a metaphor that reads more like poetic prose than its intent.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 875) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Previous research on dialogue systems generally focuses on the conversation between two participants, yet multi-party conversations which involve more than two participants within one session bring up a more complicated but realistic scenario. In real multi- party conversations, we can observe who is speaking, but the addressee information is not always explicit. In this paper, we aim to tackle the challenge of identifying all the miss- ing addressees in a conversation session. To this end, we introduce a novel who-to-whom (W2W) model which models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements.\n",
            "\n",
            "[Sample A]:\n",
            "A Model for Identifying Addressees in Multi-Party Dialogue Sessions Using Joint User and Utterance Modeling\n",
            "\n",
            "[Sample B]:\n",
            "Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A: More focused on the model itself and the improvement over existing methods.\n",
            "Sample B: Uses a more conversational tone to introduce the topic and the challenge of the problem.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 880) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "How much is 131 million US dollars? To help readers put such numbers in context, we propose a new task of automatically generating short descriptions known as perspectives, e.g. “$131 million is about the cost to employ everyone in Texas over a lunch period”. First, we collect a dataset of numeric mentions in news articles, where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity, numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neural network. Our system obtains a 15.2% F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation.\n",
            "\n",
            "[Sample A]:\n",
            "A System for Generating Contextual Descriptions of Large Numeric Values Using Compositional Methods\n",
            "\n",
            "[Sample B]:\n",
            "How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B highlights the key information \"How Much is 131 Million Dollars? \", which immediately draws attention to the number. This creates a sense of curiosity and engagement, making it more captivating.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 885) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure.\n",
            "\n",
            "[Sample A]:\n",
            "Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks\n",
            "\n",
            "[Sample B]:\n",
            "An Analysis of Architectural Factors Influencing Hierarchical and Linear Inductive Biases in Sequence-to-Sequence Neural Network Models for English Syntax Tasks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more engaging with a pun and an attention-grabbing title. It piques the reader's interest with its combination of a semantic pun (\"Syntax needs to grow on trees?\") and an informative title.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 890) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.\n",
            "\n",
            "[Sample A]:\n",
            "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models\n",
            "\n",
            "[Sample B]:\n",
            "An Empirical Comparison of Pretrained Multilingual and Monolingual Language Models Across Multiple Languages and Tasks, with a Focus on Tokenizer Effects\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "This title Sample B provides a more comprehensive comparison of pre-trained multilingual and monolingual language model performances across multiple languages and tasks, including focus on tokenizer effects. It sounds more engaging to the readers as it mentions tokenizer effects directly, which is the crux issue in the research. The sample A, on the other hand, is more focused on just tokenizer performance through fair and controlled comparisons, without mentioning tokenizer effects. Therefore, the title Sample B is slightly more engaging and descriptive.\n",
            "\n",
            "However, both titles are still quite interesting and engaging. If you must choose, Sample B might be considered slightly funnier or more attention-grabbing.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 895) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We study the perception of situational power in written dialogs in the context of organizational emails and contrast it to the power attributed by organizational hierarchy. We analyze various correlates of the perception of power in the dialog structure and language use by participants in the dialog. We also present an SVM-based machine learning system using dialog structure and lexical features to predict persons with situational power in a given communication thread.\n",
            "\n",
            "[Sample A]:\n",
            "Analysis of Situational Power Perception and Hierarchical Power in Organizational Email Communications Using Machine Learning\n",
            "\n",
            "[Sample B]:\n",
            "Who's (Really) the Boss? Perception of Situational Power in Written Interactions\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more compelling. It uses a wordplay with \"boss\" and \"really, \" making the abstract more engaging.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 900) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "A wish is “a desire or hope for something to happen.” In December 2007, people from around the world offered up their wishes to be printed on confetti and dropped from the sky during the famous New Year’s Eve “ball drop” in New York City’s Times Square. We present an in-depth analysis of this collection of wishes. We then leverage this unique resource to conduct the first study on building general “wish detectors” for natural language text. Wish detection complements traditional sentiment analysis and is valuable for collecting business intelligence and insights into the world’s wants and desires. We demonstrate the wish detectors’ effectiveness on domains as diverse as consumer product reviews and online political discussions.\n",
            "\n",
            "[Sample A]:\n",
            "May All Your Wishes Come True: A Study of Wishes and How to Recognize Them\n",
            "\n",
            "[Sample B]:\n",
            "Analyzing a Global Collection of Wishes and Developing Automatic Wish Detection in Text\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A highlights the celebratory nature of the wishes, emphasizing the hope for something to happen. It uses a catchy phrase that implies positivity and expectation.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 905) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Peer review is a core element of the scientific process, particularly in conference-centered fields such as ML and NLP. However, only few studies have evaluated its properties empirically. Aiming to fill this gap, we present a corpus that contains over 4k reviews and 1.2k author responses from ACL-2018. We quantitatively and qualitatively assess the corpus. This includes a pilot study on paper weaknesses given by reviewers and on quality of author responses. We then focus on the role of the rebuttal phase, and propose a novel task to predict after-rebuttal (i.e., final) scores from initial reviews and author responses. Although author responses do have a marginal (and statistically significant) influence on the final scores, especially for borderline papers, our results suggest that a reviewer's final score is largely determined by her initial score and the distance to the other reviewers' initial scores. In this context, we discuss the conformity bias inherent to peer reviewing, a bias that has largely been overlooked in previous research. We hope our analyses will help better assess the usefulness of the rebuttal phase in NLP conferences.\n",
            "\n",
            "[Sample A]:\n",
            "Does My Rebuttal Matter? Insights from a Major NLP Conference\n",
            "\n",
            "[Sample B]:\n",
            "An Empirical Analysis of Peer Review Dynamics and Rebuttal Impact at ACL 2018\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are informative and cover the main point of the abstract. Sample B \"An Empirical Analysis of Peer Review Dynamics and Rebuttal Impact at ACL 2018\" is more concise and highlights the empirical nature of the study, which could be seen as more fitting for a research abstract with a focus on empirical findings. Sample A \"Does My Rebuttal Matter? Insights from a Major NLP Conference\" captures the essence of the study but lacks the detail that Sample B provides. However, the title is more engaging and could work well for a title in a different context.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 910) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest that such a task requires deep language understanding, common sense, and world knowledge and thus can be a good testbed for pretrained language models and help models perform better on other tasks.\n",
            "\n",
            "[Sample A]:\n",
            "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge\n",
            "\n",
            "[Sample B]:\n",
            "A Chinese Dataset and Evaluation Framework for Computational Cant Understanding Incorporating Common Sense and World Knowledge\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is a bit fancier but still straightforward and informative. Sample A is a bit more engaging with a pun, but the main message is the same. Both are equally good in conveying the research topic and purpose.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 915) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage.\n",
            "\n",
            "[Sample A]:\n",
            "An Analysis of Inter-Metric Agreement Across Scoring Ranges in Automatic Text Summarization Evaluation\n",
            "\n",
            "[Sample B]:\n",
            "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B strikes a clever play on words by mentioning \"Metrics also Disagree,\" which captures the broader context of disagreements across various ranges. The title is more compelling and intriguing, hinting at an even deeper exploration into the nuances of evaluation metrics.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 920) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "A wish is “a desire or hope for something to happen.” In December 2007, people from around the world offered up their wishes to be printed on confetti and dropped from the sky during the famous New Year’s Eve “ball drop” in New York City’s Times Square. We present an in-depth analysis of this collection of wishes. We then leverage this unique resource to conduct the first study on building general “wish detectors” for natural language text. Wish detection complements traditional sentiment analysis and is valuable for collecting business intelligence and insights into the world’s wants and desires. We demonstrate the wish detectors’ effectiveness on domains as diverse as consumer product reviews and online political discussions.\n",
            "\n",
            "[Sample A]:\n",
            "May All Your Wishes Come True: A Study of Wishes and How to Recognize Them\n",
            "\n",
            "[Sample B]:\n",
            "Analyzing a Global Collection of Wishes and Developing Automatic Wish Detection in Text\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A emphasizes the universality and universal nature of the wishes, which is catchy and attention-grabbing. It's relatable and can evoke emotions, creating a more immediate impact.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 925) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "When learning language, infants need to break down the flow of input speech into minimal word-like units, a process best described as unsupervised bottom-up segmentation. Proposed strategies include several segmentation algorithms, but only cross-linguistically robust algorithms could be plausible candidates for human word learning, since infants have no initial knowledge of the ambient language. We report on the stability in performance of 11 conceptually diverse algorithms on a selection of 8 typologically distinct languages. The results consist evidence that some segmentation algorithms are cross-linguistically valid, thus could be considered as potential strategies employed by all infants.\n",
            "\n",
            "[Sample A]:\n",
            "A Cross-Linguistic Evaluation of Unsupervised Word Segmentation Algorithms in Infant Language Acquisition\n",
            "\n",
            "[Sample B]:\n",
            "Is Word Segmentation Child's Play in All Languages?\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and directly addresses the main point of the abstract. It fits the reader's thought process more naturally by stating the purpose at the beginning, followed by its findings. Sample B is more ambiguous and could be interpreted as asking a question, which is not the intended main focus. The title \"Cross-Linguistic Evaluation of Unsupervised Word Segmentation Algorithms in Infant Language Acquisition\" addresses the research question and the methodology well.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 930) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the handengineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.\n",
            "\n",
            "[Sample A]:\n",
            "DeepMath - Deep Sequence Models for Premise Selection\n",
            "\n",
            "[Sample B]:\n",
            "A Two-Stage Neural Sequence Model Approach for Large-Scale Premise Selection in Automated Theorem Proving\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more concise and directly specifies the core of the research, which is the two-stage neural sequence model approach for large-scale premise selection in automated theorem proving. This title is more precise and potentially more impactful, as it directly addresses the key methodological innovation of the study.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 935) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.\n",
            "\n",
            "[Sample A]:\n",
            "A Diagnostic Method for Evaluating Cross-Modal Interactions in Multimodal Machine Learning Models Using Empirical Multimodally-Additive Function Projection\n",
            "\n",
            "[Sample B]:\n",
            "Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The proposed title \"A Diagnostic Method for Evaluating Cross-Modal Interactions in Multimodal Machine Learning Models Using Empirical Multimodally-Additive Function Projection\" is straightforward and formal. It suggests a method for evaluating cross-modal interactions, which could be interesting but might not be as engaging as a humorous title.\n",
            "\n",
            "The title \"Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!\" is much funnier and more engaging. It questions the audience's intuition about the model's capabilities and adds a bit of humor, making it more appealing to readers.\n",
            "\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 940) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Layer-sequential unit-variance (LSUV) initialization – a simple method for weight initialization for deep net learning – is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.\n",
            "\n",
            "[Sample A]:\n",
            "A Method for Weight Initialization Using Layer-Sequential Unit-Variance to Improve Deep Neural Network Training and Performance\n",
            "\n",
            "[Sample B]:\n",
            "All you need is a good init\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more fitting as it introduces a method (Layer-Sequential Unit-Variance) with an emphasis on performance improvement, which aligns with the research claim. Sample B is too vague and does not clearly connect to the method or potential benefits.\n",
            "</notes>\n",
            "\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 945) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective_discourse.\n",
            "\n",
            "[Sample A]:\n",
            "A Dataset and Computational Analysis of Subjective Interpretations in English Conversational Discourse\n",
            "\n",
            "[Sample B]:\n",
            "Did they answer? Subjective acts and intents in conversational discourse\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A encapsulates the essence of the dataset and the analysis methods, highlighting its detailed focus on subjective interpretations and the computational models used for prediction. This title suggests a more academic, structured approach.\n",
            "Sample B, on the other hand, is more concise and focuses solely on the content of the dataset. It seems to imply a more casual, brief analysis.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 950) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.\n",
            "\n",
            "[Sample A]:\n",
            "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns\n",
            "\n",
            "[Sample B]:\n",
            "Development of a Gender-Balanced Corpus for Ambiguous Pronoun Resolution in Coreference Tasks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more engaging and intriguing by highlighting the potential impact of the research. Also, \"GAP\" is a play on words, implying the challenge of addressing gender bias and providing a comprehensive solution. However, Sample B, while more informative about the corpus's purpose, does not immediately capture the reader's attention in the same way as Sample A.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 955) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Neural data compression has been shown to outperform classical methods in terms of rate-distortion (RD) performance, with results still improving rapidly. At a high level, neural compression is based on an autoencoder that tries to reconstruct the input instance from a (quantized) latent representation, coupled with a prior that is used to losslessly compress these latents. Due to limitations on model capacity and imperfect optimization and generalization, such models will suboptimally compress test data in general. However, one of the great strengths of learned compression is that if the test-time data distribution is known and relatively lowentropy (e.g. a camera watching a static scene, a dash cam in an autonomous car, etc.), the model can easily be finetuned or adapted to this distribution, leading to improved RD performance. In this paper we take this concept to the extreme, adapting the full model to a single video, and sending model updates (quantized and compressed using a parameter-space prior) along with the latent representation. Unlike previous work, we finetune not only the encoder/latents but the entire model, and during finetuning take into account both the effect of model quantization and the additional costs incurred by sending the model updates. We evaluate an image compression model on I-frames (sampled at 2 fps) from videos of the Xiph dataset, and demonstrate that full-model adaptation improves RD performance by ∼ 1 dB, with respect to encoder-only finetuning.\n",
            "\n",
            "[Sample A]:\n",
            "Full-Model Instance-Level Adaptation for Neural Data Compression with Quantized Parameter Updates\n",
            "\n",
            "[Sample B]:\n",
            "Overfitting for Fun and Profit: Instance-Adaptive Data Compression\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A introduces the concept of full-model instance-level adaptation, which is more technical and potentially less engaging than Sample B. Sample B uses \"Overfitting for Fun and Profit\", which is a catchy title and emphasizes the fun aspect associated with the topic.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 960) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In recent years, learning user preferences has received significant attention. A shortcoming of existing learning to rank work lies in that they do not take into account the multilevel hierarchies from social choice to individuals. In this paper, we propose a multi-level model which learns both the common preference or utility function over the population based on features of alternatives to-be-compared, and preferential diversity functions conditioning on user categories. Such a multi-level model, enables us to simultaneously learn a coarse-grained social preference function together with a fine-grained personalized diversity. It provides us prediction power for the choices of new users on new alternatives. The key algorithm in this paper is based on Split Linearized Bregman Iteration (SplitLBI) algorithm which generates a dynamic path from the common utility to personalized preferential diversity, at different levels of sparsity on personalization. A synchronized parallel version of SplitLBI is proposed to meet the needs of fast analysis of large-scale data. The validity of the methodology are supported by experiments with both simulated and real-world datasets such as movie and dining restaurant ratings which provides us a coarse-to-fine grained preference learning.\n",
            "\n",
            "[Sample A]:\n",
            "Who Likes What? — SplitLBI in Exploring Preferential Diversity of Ratings\n",
            "\n",
            "[Sample B]:\n",
            "A Multi-Level SplitLBI Approach for Hierarchical User Preference and Diversity Modeling in Rating Data\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is a more fitting title as it more directly addresses the focus of the research. It highlights the use of SplitLBI in exploring hierarchical user preference and diversity modeling, which better encapsulates the research objectives and the approach described in the abstract.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 965) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Automatic question generation (QG) is a useful yet challenging task in NLP. Recent neural network-based approaches represent the state-of-the-art in this task. In this work, we attempt to strengthen them significantly by adopting a holistic and novel generator-evaluator framework that directly optimizes objectives that reward semantics and structure. The generator is a sequence-to-sequence model that incorporates the structure and semantics of the question being generated. The generator predicts an answer in the passage that the question can pivot on. Employing the copy and coverage mechanisms, it also acknowledges other contextually important (and possibly rare) keywords in the passage that the question needs to conform to, while not redundantly repeating words. The evaluator model evaluates and assigns a reward to each predicted question based on its conformity to the structure of ground-truth questions. We propose two novel QG-specific reward functions for text conformity and answer conformity of the generated question. The evaluator also employs structure-sensitive rewards based on evaluation measures such as BLEU, GLEU, and ROUGE-L, which are suitable for QG. In contrast, most of the previous works only optimize the cross-entropy loss, which can induce inconsistencies between training (objective) and testing (evaluation) measures. Our evaluation shows that our approach significantly outperforms state-of-the-art systems on the widely-used SQuAD benchmark as per both automatic and human evaluation.\n",
            "\n",
            "[Sample A]:\n",
            "Putting the Horse before the Cart: A Generator-Evaluator Framework for Question Generation from Text\n",
            "\n",
            "[Sample B]:\n",
            "A Generator-Evaluator Architecture Optimizing Structure and Semantic Objectives for Neural Question Generation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A introduces a metaphor that plays on the confusion between the terms \"horse\" and \"cart,\" making the title more engaging and memorable. However, Sample B is more concise and to the point, which makes it a better choice for a research abstract.\n",
            "</notes>\n",
            "\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 970) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimization. Our experiments in model-based reinforcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamental chaos-like nature of long chains of nonlinear computations. Not only do the magnitudes of the gradients become large, the direction of the gradients becomes essentially random. We show that reparameterization gradients suffer from the problem, while likelihood ratio gradients are robust. Using our insights, we develop a model-based policy search framework, Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is easily extensible, and allows for almost arbitrary models and policies, while simultaneously matching the performance of previous data-efficient learning algorithms. Finally, we invent the total propagation algorithm, which efficiently computes a union over all pathwise derivative depths during a single backwards pass, automatically giving greater weight to estimators with lower variance, sometimes improving over reparameterization gradients by 10 times.\n",
            "\n",
            "[Sample A]:\n",
            "A Framework for Model-Based Policy Search Addressing Chaos-Induced Gradient Instability in Reinforcement Learning\n",
            "\n",
            "[Sample B]:\n",
            "PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more concise and direct in its explanation of the problem, emphasizing \"curse of chaos\". This likely makes it more memorable and impactful, hence potentially more funnier or more engaging.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 975) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This paper describes the CMU Let’s Go!! bus information system, an experimental system designed to study the use of spoken dialogue interfaces by non-native speakers. The differences in performance of the speech recognition and language understanding modules of the system when confronted with native and non-native spontaneous speech are analyzed. Focus is placed on the linguistic mismatch between the user input and the system’s expectations, and on its implications in terms of language modeling and parsing performance. The effect of including non-native data when building the speech recognition and language understanding modules is discussed. In order to close the gap between non-native and native input, a method is proposed to automatically generate confirmation prompts that are both close to the user’s input and covered by the system’s language model and grammar, in order to help the user acquire idiomatic expressions appropriate to the task.\n",
            "\n",
            "[Sample A]:\n",
            "Non-Native Users in the Let's Go!! Spoken Dialogue System: Dealing with Linguistic Mismatch\n",
            "\n",
            "[Sample B]:\n",
            "Analysis of Speech Recognition and Language Understanding for Native and Non-Native Speakers in a Bus Information Spoken Dialogue System\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The titles are somewhat comparable in focus, with Sample A focusing on the use of the system for the task of dealing with linguistic mismatches between user input and system expectations due to non-native speakers, while Sample B focuses on the analysis of speech recognition and language understanding for both native and non-native speakers in a bus information system. Sample A is more concise and uses a humorous tone, as the title directly references the \"Let's Go!!\" theme.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 980) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We conduct a large-scale, systematic study to evaluate the existing evaluation methods for natural language generation in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the ranking of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for natural language generation. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find lexical diversity an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of natural language generation systems.\n",
            "\n",
            "[Sample A]:\n",
            "A Systematic Comparison of Human and Automated Evaluation Metrics for Neural Language Generation in Online Product Reviews\n",
            "\n",
            "[Sample B]:\n",
            "Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is engaging and memorable with its title, making it more likely to capture attention and intrigue potential readers. It encapsulates the essence of the study succinctly, highlighting the pivotal role of the judges in the study, which immediately piques interest.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 985) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We extend classic review mining work by building a binary classifier that predicts whether a review of a documentary film was written by an expert or a layman with 90.70% accuracy (F1 score), and compare the characteristics of the predicted classes. A variety of standard lexical and syntactic features was used for this supervised learning task. Our results suggest that experts write comparatively lengthier and more detailed reviews that feature more complex grammar and a higher diversity in their vocabulary. Layman reviews are more subjective and contextualized in peoples' everyday lives. Our error analysis shows that laymen are about twice as likely to be mistaken as experts than vice versa. We argue that the type of author might be a useful new feature for improving the accuracy of predicting the rating, helpfulness and authenticity of reviews. Finally, the outcomes of this work might help researchers and practitioners in the field of impact assessment to gain a more fine-grained understanding of the perception of different types of media consumers and reviewers of a topic, genre or information product.\n",
            "\n",
            "[Sample A]:\n",
            "A Supervised Learning Approach to Classifying Expert and Layman Reviews of Documentary Films Using Linguistic Features\n",
            "\n",
            "[Sample B]:\n",
            "Says Who{\\ldots}? Identification of Expert versus Layman Critics' Reviews of Documentary Films\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more intriguing and concise, drawing attention to the identification aspect. It sets a more curious tone, as if setting the tone to a detective story. While Sample A is thorough and methodical, it feels more academic and less engaging. The phrase \"Says Who{\\ldots? \" is quite catchy and sets a playful, perhaps even humorous, tone in contrast to the somewhat formal and content-driven tone of Sample A.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 990) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Scientific papers revolve around citations, and for many discourse level tasks one needs to know whose work is being talked about at any point in the discourse. In this paper, we introduce the scientific attribution task, which links different linguistic expressions to citations. We discuss the suitability of different evaluation metrics and evaluate our classification approach to deciding attribution both intrinsically and in an extrinsic evaluation where information about scientific attribution is shown to improve performance on Argumentative Zoning, a rhetorical classification task.\n",
            "\n",
            "[Sample A]:\n",
            "A Study on Scientific Attribution: Linking Linguistic Expressions to Citations and Evaluating Their Impact on Argumentative Zoning\n",
            "\n",
            "[Sample B]:\n",
            "Whose Idea Was This, and Why Does it Matter? Attributing Scientific Work to Citations\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and highlights the thematic focus of the study, which is more appropriate for a title in the realm of scientific attribution and citation analysis. It conveys curiosity and intrigue about the impact on the task, Argumentative Zoning, which aligns with the abstract's discussion. Sample A is too broad and lacks specificity and impact.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 995) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The Xenotext Experiment implants poetry into an extremophile’s DNA, and uses that DNA to generate new poetry in a protein form. The molecular machinery of life requires that these two poems encipher each other under a symmetric substitution cipher. We search for ciphers which permit writing under the Xenotext constraints, incorporating ideas from cipher-cracking algorithms, and using n-gram data to assess a cipher’s “writability”. Our algorithm, Beam Verse, is a beam search which uses new heuristics to navigate the cipher-space. We find thousands of ciphers which score higher than successful ciphers used to write Xenotext constrained texts.\n",
            "\n",
            "[Sample A]:\n",
            "Applying Beam Search Algorithms to Identify Symmetric Substitution Ciphers for Biopoetic Encoding in the Xenotext Experiment\n",
            "\n",
            "[Sample B]:\n",
            "Poet Admits // Mute Cypher: Beam Search to find Mutually Enciphering Poetic Texts\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and straightforward in its proposal of using Beam Search Algorithms to identify symmetric substitution ciphers for biopoetic encoding. Sample B, while clever in its title, is more ambiguous and somewhat disjointed. \n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1000) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research. We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines.\n",
            "\n",
            "[Sample A]:\n",
            "Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?\n",
            "\n",
            "[Sample B]:\n",
            "Ethical Considerations and Decision-Making Processes for Appropriate Uses of NLP Research, with a Focus on Automatic Legal Sentencing\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more provocative and engaging due to its attention-grabbing title. It uses alliteration (\"Give Me Convenience and Give Her Death\") which can be perceived as emotionally charged and memorable, promising the reader a strong and potentially controversial topic.\n",
            "Sample B is more clinical and focused, which might seem less impactful to casual readers.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1005) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for \"connotations\" to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.\n",
            "\n",
            "[Sample A]:\n",
            "ENTRUST: Argument Reframing with Language Models and Entailment\n",
            "\n",
            "[Sample B]:\n",
            "A Dataset and Method for Controllable Positive Lexical Reframing with Entailment Preservation in Arguments\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A highlights the transformative nature of using language models for reframing, emphasizing the entwinement of controllable text generation and entailment. It resonates with the idea that language models can be used to reframe arguments positively.\n",
            "Sample B, while informative, focuses on a specific aspect of lexical framing without the broader context that Sample A provides.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1010) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers, boosting and SVMs are applied to language processing tasks, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various error correction mechanisms have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it. This problem is especially severe if the base classifier has already\n",
            "\n",
            "[Sample A]:\n",
            "An Empirical Analysis of the Effectiveness of Error Correction Methods in Language Processing Classifiers\n",
            "\n",
            "[Sample B]:\n",
            "Why Nitpicking Works: Evidence for Occam's Razor in Error Correctors\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "<SampleA> compares the effectiveness of error correction methods in boosting, SVMs, and maximum entropy classifiers. It suggests that most error correction methods might not consistently improve performance on unseen data. This is particularly true for classifiers like SVMs, which already have high accuracies.\n",
            "<SampleB> suggests that error corruption methods work on small issues and does not discuss the impact of classifiers like SVMs or boosting in the context of language processing.\n",
            "</notes>\n",
            "\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1015) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.\n",
            "\n",
            "[Sample A]:\n",
            "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n",
            "\n",
            "[Sample B]:\n",
            "A Methodology for Detecting and Reducing Gender Bias in Word Embeddings\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are informative, but Sample A is more engaging and intriguing. Sample B, while technically correct, lacks the playful tone of Sample A. Sample A captures the humorous notion from the abstract that it is a woman’s job to be a homemaker, which can be misinterpreted humorously. The word choice \"Debiasing Word Embeddings\" also enhances the sense of irony, making the title more appealing and memorable.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1020) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.\n",
            "\n",
            "[Sample A]:\n",
            "A Comprehensive Evaluation of Textual Representations Addressing Lexical Composition Effects\n",
            "\n",
            "[Sample B]:\n",
            "Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B captures the challenge of the current research better. The phrase \"Still a Pain in the Neck\" directly relates to the discomfort associated with the nature of the problem (lexical composition) and implies the ongoing difficulty that the study aims to address. This captures the essence of the research better.\n",
            "</notes>\n",
            "\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1025) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the classic hidden Markov model with the contextual representation power of pre-trained language models. Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations. We further refine CHMM with an alternate-training approach (CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERT-NER's output is regarded as an additional weak source to train the CHMM in return. Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins.\n",
            "\n",
            "[Sample A]:\n",
            "BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition\n",
            "\n",
            "[Sample B]:\n",
            "A Conditional Hidden Markov Model Approach Enhanced with BERT for Multi-Source Weakly Supervised Named Entity Recognition\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and uses \"BERTifying\" which is more of a pun on BERT. It's more engaging and interesting. Sample B is more straightforward and doesn't contain a pun.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1030) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In this paper we investigate a new problem of identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy.\n",
            "\n",
            "[Sample A]:\n",
            "Which Side are You on? Identifying Perspectives at the Document and Sentence Levels\n",
            "\n",
            "[Sample B]:\n",
            "Statistical Modeling for Automatic Identification of Document and Sentence-Level Perspectives\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more engaging and has a stronger hook by using a question and providing a surprising answer. It also matches the focus of the abstract more easily. Sample B is more concise and technical, focusing on the methodology rather than the content. Both titles are effective, but Sample A is fonder and more interesting to a general audience.\n",
            "</notes>\n",
            "\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1035) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.\n",
            "\n",
            "[Sample A]:\n",
            "Thumbs up? Sentiment Classification using Machine Learning Techniques\n",
            "\n",
            "[Sample B]:\n",
            "A Comparative Study of Machine Learning Methods for Sentiment Analysis of Movie Reviews\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is simpler and focuses on the positive aspect only, while Sample B provides a more comprehensive and comparative analysis. Sample B is therefore funnier as it delves into the core of the problem and the limitations of machine learning techniques, making it more engaging and informative.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1040) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Dense embedding models are commonly deployed in commercial search engines, wherein all the document vectors are pre-computed, and near-neighbor search (NNS) is performed with the query vector to find relevant documents. However, the bottleneck of indexing a large number of dense vectors and performing an NNS hurts the query time and accuracy of these models. In this paper, we argue that highdimensional and ultra-sparse embedding is a significantly superior alternative to dense low-dimensional embedding for both query efficiency and accuracy. Extreme sparsity eliminates the need for NNS by replacing them with simple lookups, while its high dimensionality ensures that the embeddings are informative even when sparse. However, learning extremely high dimensional embeddings leads to blow up in the model size. To make the training feasible, we propose a partitioning algorithm that learns such high dimensional embeddings across multiple GPUs without any communication. This is facilitated by our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random (SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal by design, while the query vectors are learned and sparse. We theoretically prove that our way of one-sided learning is equivalent to learning both query and label embeddings. With these unique properties, we can successfully train 500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books and multi-label classification on the three largest public datasets. We achieve superior precision and recall compared to the respective state-of-the-art baselines for each task with up to 10× faster speed.\n",
            "\n",
            "[Sample A]:\n",
            "SOLAR: Sparse Orthogonal Learned and Random Embeddings\n",
            "\n",
            "[Sample B]:\n",
            "Ultra-High Dimensional Sparse Embeddings with Partitioned Training for Efficient Search and Classification\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The first title is simple and direct, focusing on the term \"SOLAR\" which is specific and memorable. It uses less technical jargon, making it easier to understand for a general audience.\n",
            "The second title is more complex, using technical terms like \"Ultra-High Dimensional Sparse Embeddings\" and \"Partitioned Training.\" This title is appropriate for the academic context but might be too specific and might not be as appealing to a non-expert audience.\n",
            "\n",
            "The abstract discusses the benefits of using high-dimensional and ultra-sparse embeddings over dense embeddings, mentioning how it improves query efficiency and accuracy. However, both titles are relatively straightforward and do not highlight the benefits as prominently as they could. \n",
            "\n",
            "Ultimately, the first title is more straightforward and better conveys the essence of the paper.\n",
            "</notes>\n",
            "\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1045) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.\n",
            "\n",
            "[Sample A]:\n",
            "Vancouver Welcomes You! Minimalist Location Metonymy Resolution\n",
            "\n",
            "[Sample B]:\n",
            "A Neural Approach and Dataset for Location-Based Metonymy Resolution in Natural Language Processing\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more concise and directly states the focus of the research, providing clarity on what the methods and dataset are for. It also mentions \"Natural Language Processing,\" which could be a potential benefit for a broader audience. However, Sample A has a more enthusiastic and impactful tone, which might be appealing to those interested in the metonymic aspect of named entities.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1050) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We present an unsupervised model for inducing signed social networks from the content exchanged across network edges. Inference in this model solves three problems simultaneously: (1) identifying the sign of each edge; (2) characterizing the distribution over content for each edge type; (3) estimating weights for triadic features that map to theoretical models such as structural balance. We apply this model to the problem of inducing the social function of address terms, such as Madame, comrade, and dude. On a dataset of movie scripts, our system obtains a coherent clustering of address terms, while at the same time making intuitively plausible judgments of the formality of social relations in each film. As an additional contribution, we provide a bootstrapping technique for identifying and tagging address terms in dialogue.1\n",
            "\n",
            "[Sample A]:\n",
            "Unsupervised Induction of Address Term Formality and Signed Social Networks from Movie Dialogue\n",
            "\n",
            "[Sample B]:\n",
            "\"You're Mr. Lebowski, I'm the Dude\": Inducing Address Term Formality in Signed Social Networks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more fun and attention-grabbing. It uses humor (\"a clever pun\"), making it appealing to a broader audience.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1055) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "A major challenge for video captioning is to combine audio and visual cues. Existing multi-modal fusion methods have shown encouraging results in video understanding. However, the temporal structures of multiple modalities at different granularities are rarely explored, and how to selectively fuse the multi-modal representations at different levels of details remains uncharted. In this paper, we propose a novel hierarchically aligned cross-modal attention (HACA) framework to learn and selectively fuse both global and local temporal dynamics of different modalities. Furthermore, for the first time, we validate the superior performance of the deep audio features on the video captioning task. Finally, our HACA model significantly outperforms the previous best systems and achieves new state-of-the-art results on the widely used MSR-VTT dataset.\n",
            "\n",
            "[Sample A]:\n",
            "Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning\n",
            "\n",
            "[Sample B]:\n",
            "Hierarchically Aligned Cross-Modal Attention Framework for Multi-Granular Fusion of Audio and Visual Features in Video Captioning\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B focuses on HACA's ability to fuse across multiple levels of granularity, which implies a more comprehensive approach. Sample A, on the other hand, is more specific to the cross-modal attention aspect and how it works. Both titles are interesting and could potentially attract interest but Sample B might seem a bit more straightforward in its approach.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1060) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We describe a synchronous parsing algorithm that is based on two successive monolingual parses of an input sentence pair. Although the worst-case complexity of this algorithm is and must be O(n) for binary SCFGs, its average-case run-time is far better. We demonstrate that for a number of common synchronous parsing problems, the two-parse algorithm substantially outperforms alternative synchronous parsing strategies, making it efficient enough to be utilized without resorting to a pruned search.\n",
            "\n",
            "[Sample A]:\n",
            "Two monolingual parses are better than one (synchronous parse)\n",
            "\n",
            "[Sample B]:\n",
            "An Efficient Synchronous Parsing Algorithm Based on Successive Monolingual Parses\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is the more interesting and concise title. It directly states the core of the research without unnecessary details about run-time complexity and worst-case scenarios. It also conveys the main idea of relying on two monolingual parses for synchronous parsing, which is the focus of the research.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1065) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We investigate the use of language in food writing, specifically on restaurant menus and in customer reviews. Our approach is to build predictive models of concrete external variables, such as restaurant menu prices. We make use of a dataset of menus and customer reviews for thousands of restaurants in several U.S. cities. By focusing on prediction tasks and doing our analysis at scale, our methodology allows quantitative, objective measurements of the words and phrases used to describe food in restaurants. We also explore interactions in language use between menu prices and sentiment as expressed in user reviews.\n",
            "\n",
            "[Sample A]:\n",
            "Word Salad: Relating Food Prices and Descriptions\n",
            "\n",
            "[Sample B]:\n",
            "A Quantitative Analysis of Language in Restaurant Menus and Customer Reviews for Predicting Menu Prices and Sentiment Associations\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The Abstract highlights the use of quantification in analyzing food language. While Sample A focuses on the relativity between food pricing and description, Sample B places emphasis on the analytical methods applied to such relationship. The style of Sample A might be more casual and relatable, whereas Sample B is more formal and analytical.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1070) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Recent work has shown that pre-trained Transformers obtain remarkable performance on many natural language processing tasks including automatic summarization. However, most work has focused on (relatively) data-rich single-document summarization settings. In this paper, we explore highly-abstractive multi-document summarization where the summary is explicitly conditioned on a user-given topic statement or question. We compare the summarization quality produced by three state-of-the-art transformer-based models: BART, T5, and PEGASUS. We report the performance on four challenging summarization datasets: three from the general domain and one from consumer health in both zero-shot and few-shot learning settings. While prior work has shown significant differences in performance for these models on standard summarization tasks, our results indicate that with as few as 10 labeled examples there is no statistically significant difference in summary quality, suggesting the need for more abstractive benchmark collections when determining state-of-the-art.\n",
            "\n",
            "[Sample A]:\n",
            "Flight of the PEGASUS? Comparing Transformers on Few-shot and Zero-shot Multi-document Abstractive Summarization\n",
            "\n",
            "[Sample B]:\n",
            "An Empirical Comparison of Transformer-based Models for Multi-Document Abstractive Summarization in Few-Shot and Zero-Shot Settings\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more concise and to the point, using a question format that is immediately engaging and draws the reader’s attention. The comparative nature of the question also hints at the academic rigor of the study, making it clear that the research is focused on comparing models. This is likely intended to make the paper seem more intriguing and interesting to a broader audience.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1075) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Most state of the art approaches for machine transliteration are data driven and require significant parallel names corpora between languages. As a result, developing transliteration functionality among n languages could be a resource intensive task requiring parallel names corpora in the order of nC2. In this paper, we explore ways of reducing this high resource requirement by leveraging the available parallel data between subsets of the n languages, transitively. We propose, and show empirically, that reasonable quality transliteration engines may be developed between two languages, X and Y , even when no direct parallel names data exists between them, but only transitively through language Z . Such systems alleviate the need for O(nC2) corpora, significantly. In addition we show that the performance of such transitive transliteration systems is in par with direct transliteration systems, in practical applications, such as CLIR systems.\n",
            "\n",
            "[Sample A]:\n",
            "Empirical Analysis of Transitive Machine Transliteration Using Bridge Languages\n",
            "\n",
            "[Sample B]:\n",
            "Everybody loves a rich cousin: An empirical study of transliteration through bridge languages\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B captures the essence of the research by highlighting the common theme of \"rich cousin\" in the title. This makes it more relatable and engaging to the general audience. It also creates a sense of curiosity and intrigue about the study's findings.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1080) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "A single-elimination (SE) tournament is a popular way to select a winner in both sports competitions and in elections. A natural and well-studied question is the tournament fixing problem (TFP): given the set of all pairwise match outcomes, can a tournament organizer rig an SE tournament by adjusting the initial seeding so that their favorite player wins? We prove new sufficient conditions on the pairwise match outcome information and the favorite player, under which there is guaranteed to be a seeding where the player wins the tournament. Our results greatly generalize previous results. We also investigate the relationship between the set of players that can win an SE tournament under some seeding (so called SE winners) and other traditional tournament solutions. In addition, we generalize and strengthen prior work on probabilistic models for generating tournaments. For instance, we show that every player in an n player tournament generated by the Condorcet Random Model will be an SE winner even when the noise is as small as possible, p = Θ(lnn/n); prior work only had such results for p ≥ Ω(lnn/n). We also establish new results for significantly more general generative models.\n",
            "\n",
            "[Sample A]:\n",
            "Sufficient Conditions and Generative Models for Player Winning Potential in Single-Elimination Tournaments\n",
            "\n",
            "[Sample B]:\n",
            "Who Can Win a Single-Elimination Tournament?\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is much simpler and directly addresses the main topic of the abstract, which is the winners in a single-elimination tournament. It doesn't incorporate the research details or theoretical results which are the core focus. Sample A, on the other hand, is more complex and covers the entire content of the abstract, including the research methods, theoretical findings, and new insights.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1085) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Political debates offer a rare opportunity for citizens to compare the candidates' positions on the most controversial topics of the campaign. Thus they represent a natural application scenario for Argument Mining. As existing research lacks solid empirical investigation of the typology of argument components in political debates, we fill this gap by proposing an Argument Mining approach to political debates. We address this task in an empirical manner by annotating 39 political debates from the last 50 years of US presidential campaigns, creating a new corpus of 29k argument components, labeled as premises and claims. We then propose two tasks: (1) identifying the argumentative components in such debates, and (2) classifying them as premises and claims. We show that feature-rich SVM learners and Neural Network architectures outperform standard baselines in Argument Mining over such complex data. We release the new corpus USElecDeb60To16 and the accompanying software under free licenses to the research community.\n",
            "\n",
            "[Sample A]:\n",
            "A Corpus and Empirical Approach for Argument Mining in 50 Years of US Presidential Campaign Debates\n",
            "\n",
            "[Sample B]:\n",
            "Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and funnier as it uses a playful tone. The use of the exclamation mark and the phrase \"Yes, we can!\" add a sense of excitement and enthusiasm. It also makes the abstract more relatable, drawing in the reader with a lighthearted approach.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1090) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Automated short-answer grading is key to help close the automation loop for large-scale, computerised testing in education. A wide range of features on different levels of linguistic processing has been proposed so far. We investigate the relative importance of the different types of features across a range of standard corpora (both from a language skill and content assessment context, in English and in German). We find that features on the lexical, text similarity and dependency level often suffice to approximate full-model performance. Features derived from semantic processing particularly benefit the linguistically more varied answers in content assessment corpora.\n",
            "\n",
            "[Sample A]:\n",
            "A Comparative Study on Feature Types for Automated Short-Answer Grading in English and German Corpora\n",
            "\n",
            "[Sample B]:\n",
            "Get Semantic With Me! The Usefulness of Different Feature Types for Short-Answer Grading\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is a straightforward but clear title. Sample B is catchy and has a double meaning related to semantics, making it very punny. Sample B is a more effective hook that could grab the attention of readers.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1095) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance. In this work, we give a thorough analysis of the “BO + neural predictor” framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition function optimization. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.\n",
            "\n",
            "[Sample A]:\n",
            "BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search\n",
            "\n",
            "[Sample B]:\n",
            "An Empirical Analysis of Bayesian Optimization with Neural Predictors for Neural Architecture Search and a Novel Path-based Encoding Method\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and concise. It highlights the novelty of the work, the use of neural predictors, and introduces a new encoding method. The abstract's focus on quantifiable improvements (state-of-the-art performance) also makes it more impactful.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1100) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This paper explores the task of leveraging typology in the context of cross-lingual dependency parsing. While this linguistic information has shown great promise in pre-neural parsing, results for neural architectures have been mixed. The aim of our investigation is to better understand this state-of-the-art. Our main findings are as follows: 1) The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological dimensions in a compositional manner; 2) Typology consistent with the actual corpus statistics yields better transfer performance; 3) Typological similarity is only a rough proxy of cross-lingual transferability with respect to parsing.\n",
            "\n",
            "[Sample A]:\n",
            "Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers\n",
            "\n",
            "[Sample B]:\n",
            "An Investigation of Typology Utilization in Cross-Lingual Neural Dependency Parsing\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "A more engaging title would be one that captures the innovative aspects and the challenges of the task. An example of a more engaging title could be \"Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers\". This title not only highlights the innovative aspects but also captures the attention of those interested in the field.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1105) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.\n",
            "\n",
            "[Sample A]:\n",
            "Methods for Automatic Extractive and Abstractive Caption Generation for News Images Using Probabilistic Annotation\n",
            "\n",
            "[Sample B]:\n",
            "How Many Words Is a Picture Worth? Automatic Caption Generation for News Images\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is funnier by setting the tone to a playful question. However, Sample A is more directly related to the content of the abstract which focuses on automatic caption generation for news images.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1110) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Personalized conversation models (PCMs) generate responses according to speaker preferences. Existing personalized conversation tasks typically require models to extract speaker preferences from user descriptions or their conversation histories, which are scarce for newcomers and inactive users. In this paper, we propose a few-shot personalized conversation task with an auxiliary social network. The task requires models to generate personalized responses for a speaker given a few conversations from the speaker and a social network. Existing methods are mainly designed to incorporate descriptions or conversation histories. Those methods can hardly model speakers with so few conversations or connections between speakers. To better cater for newcomers with few resources, we propose a personalized conversation model (PCM) that learns to adapt to new speakers as well as enabling new speakers to learn from resource-rich speakers. Particularly, based on a meta-learning based PCM, we propose a task aggregator (TA) to collect other speakers’ information from the social network. The TA provides prior knowledge of the new speaker in its meta-learning. Experimental results show our methods outperform all baselines in appropriateness, diversity, and consistency with speakers.\n",
            "\n",
            "[Sample A]:\n",
            "Incorporating Social Network Data into Few-Shot Personalized Conversation Models for New and Low-Resource Users\n",
            "\n",
            "[Sample B]:\n",
            "Learning from My Friends: Few-Shot Personalized Conversation Systems via Social Networks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more engaging and relatable with the wordplay \"Learning from My Friends\", which could be seen as a personal anecdote about the speaker learning from their friends. The title is clever and captures the essence of the proposed method. \"Incorporating Social Network Data into Few-Shot Personalized Conversation Models\" sounds more academic and technical, which might be off-putting to some readers.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1115) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).\n",
            "\n",
            "[Sample A]:\n",
            "An Investigation of Gender Bias in Speech Translation Using the MuST-SHE Corpus Across English-Italian and English-French\n",
            "\n",
            "[Sample B]:\n",
            "Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is much funnier and more engaging. It creates suspense (\"Gender in Danger? \"), suggesting potential threats or risks in an audio-based translation scenario.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1120) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We propose a Laplacian structured sparsity model to study computational branding analytics. To do this, we collected customer reviews from Starbucks, Dunkin’ Donuts, and other coffee shops across 38 major cities in the Midwest and Northeastern regions of USA. We study the brand related language use through these reviews, with focuses on the brand satisfaction and gender factors. In particular, we perform three tasks: automatic brand identification from raw text, joint brand-satisfaction prediction, and joint brandgender-satisfaction prediction. This work extends previous studies in text classification by incorporating the dependency and interaction among local features in the form of structured sparsity in a log-linear model. Our quantitative evaluation shows that our approach which combines the advantages of graphical modeling and sparsity modeling techniques significantly outperforms various standard and stateof-the-art text classification algorithms. In addition, qualitative analysis of our model reveals important features of the language uses associated with the specific brands.\n",
            "\n",
            "[Sample A]:\n",
            "A Laplacian Structured Sparsity Approach for Analyzing Brand-Related Language in Customer Reviews\n",
            "\n",
            "[Sample B]:\n",
            "This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and interesting. It uses a metaphorical approach to describe the purpose of the study, making it more appealing to readers.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1125) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In politics, neologisms are frequently invented for partisan objectives. For example, \"undocumented workers\" and \"illegal aliens\" refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation. In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., \"immigrants\" vs. \"aliens\", \"estate tax\" vs. \"death tax\") move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.\n",
            "\n",
            "[Sample A]:\n",
            "A Neural Approach to Disentangling Denotative and Connotative Representations in Political Neologisms\n",
            "\n",
            "[Sample B]:\n",
            "Are \"Undocumented Workers\" the Same as \"Illegal Aliens\"? Disentangling Denotation and Connotation in Vector Spaces\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more concise and direct in its approach to the topic, aiming specifically for denotative and connotative disentanglement in political neologisms, which is a slightly different angle compared to Sample B.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1130) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Our goal is to explain the effects of perturbations in procedural text, e.g., given a passage describing a rabbit's life cycle, explain why illness (the perturbation) may reduce the rabbit population (the effect). Although modern systems are able to solve the original prediction task well (e.g., illness results in less rabbits), the explanation task - identifying the causal chain of events from perturbation to effect - remains largely unaddressed, and is the goal of this research. We present QUARTET, a system that constructs such explanations from paragraphs, by modeling the explanation task as a multitask learning problem. QUARTET constructs explanations from the sentences in the procedural text, achieving ~18 points better on explanation accuracy compared to several strong baselines on a recent process comprehension benchmark. On an end task on this benchmark, we show a surprising finding that good explanations do not have to come at the expense of end task performance, in fact leading to a 7% F1 improvement over SOTA.\n",
            "\n",
            "[Sample A]:\n",
            "A Multitask Learning Approach for Explaining the Effects of Perturbations in Procedural Text\n",
            "\n",
            "[Sample B]:\n",
            "What-if I ask you to explain: Explaining the effects of perturbations in procedural text\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is a direct and straightforward title that clearly states the focus of the research: explaining effects of perturbations in procedural text.\n",
            "Sample B is slightly more conversational and uses the \"what-if\" construct, which could seem more casual.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1135) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP. However, many researchers wonder whether these models can maintain their dominance forever. Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs. We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA. Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs. We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. Our source code and models are available at https://github.com/nict-wisdom/bertac.\n",
            "\n",
            "[Sample A]:\n",
            "BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks\n",
            "\n",
            "[Sample B]:\n",
            "A Study on Combining Adversarially Pretrained CNNs with Transformer-based Language Models for Improved Performance on NLP Benchmark Tasks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is Funnier as it adds a touch of humor with \"Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks.\"\n",
            "</notes>\n",
            "\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1140) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Annotation quality control is a critical aspect for building reliable corpora through linguistic annotation. In this study, we present a simple but powerful quality control method using two-step reason selection. We gathered sentential annotations of local acceptance and three related attributes through a crowdsourcing platform. For each attribute, the reason for the choice of the attribute value is selected in a two-step manner. The options given for reason selection were designed to facilitate the detection of a nonsensical reason selection. We assume that a sentential annotation that contains a nonsensical reason is less reliable than the one without such reason. Our method, based solely on this assumption, is found to retain the annotations with satisfactory quality out of the entire annotations mixed with those of low quality.\n",
            "\n",
            "[Sample A]:\n",
            "A Two-Step Reason Selection Method for Improving Annotation Quality of Local Acceptability and Related Attributes in News Editorials\n",
            "\n",
            "[Sample B]:\n",
            "Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is funnier because it uses a question mark and makes the title engaging.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1145) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "An important mobile health (mHealth) task is the use of multimodal data, such as sensor streams and self-report, to construct interpretable timeto-event predictions of, for example, lapse to alcohol or illicit drug use. Interpretability of the prediction model is important for acceptance and adoption by domain scientists, enabling model outputs and parameters to inform theory and guide intervention design. Temporal latent state models are therefore attractive, and so we adopt the continuous time hidden Markov model (CT-HMM) due to its ability to describe irregular arrival times of event data. Standard CTHMMs, however, are not specialized for predicting the time to a future event, the key variable for mHealth interventions. Also, standard emission models lack a sufficiently rich structure to describe multimodal data and incorporate domain knowledge. We present iSurvive, an extension of classical survival analysis to a CT-HMM. We present a parameter learning method for GLM emissions and survival model fitting, and present promising results on both synthetic data and an mHealth drug use dataset.\n",
            "\n",
            "[Sample A]:\n",
            "iSurvive: An Interpretable, Event-time Prediction Model for mHealth\n",
            "\n",
            "[Sample B]:\n",
            "A Continuous-Time Hidden Markov Model Extension Incorporating Survival Analysis for Predicting Event Timing in Mobile Health Using Multimodal Data\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is Funnier: \"iSurvive: An Interpretable, Event-time Prediction Model for mHealth\" is more engaging and has a catchy name.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1150) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This paper describes a language-independent, scalable system for both challenges of crossdocument co-reference: name variation and entity disambiguation. We provide system results from the ACE 2008 evaluation in both English and Arabic. Our English system’s accuracy is 8.4% relative better than an exact match baseline (and 14.2% relative better over entities mentioned in more than one document). Unlike previous evaluations, ACE 2008 evaluated both name variation and entity disambiguation over naturally occurring named mentions. An information extraction engine finds document entities in text. We describe how our architecture designed for the 10K document ACE task is scalable to an even larger corpus. Our cross-document approach uses the names of entities to find an initial set of document entities that could refer to the same real world entity and then uses an agglomerative clustering algorithm to disambiguate the potentially co-referent document entities. We analyze how different aspects of our system affect performance using ablation studies over the English evaluation set. In addition to evaluating cross-document coreference performance, we used the results of the cross-document system to improve the accuracy of within-document extraction, and measured the impact in the ACE 2008 withindocument evaluation.\n",
            "\n",
            "[Sample A]:\n",
            "A Language-Independent Scalable System for Cross-Document Co-Reference Resolution Involving Name Variation and Entity Disambiguation with Empirical Evaluation on ACE 2008 Tasks\n",
            "\n",
            "[Sample B]:\n",
            "Who is Who and What is What: Experiments in Cross-Document Co-Reference\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B stands out in being catchy and intriguing, hinting at its content with minimal effort. It's engaging and could capture the reader's interest early on.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1155) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, which allow for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective are capable of encoding. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence.\n",
            "\n",
            "[Sample A]:\n",
            "Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models\n",
            "\n",
            "[Sample B]:\n",
            "Evaluating Neural Language Models' Ability to Capture Discourse and Dialogue Coherence Using Specialized Test Suites\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is clever and engaging in its question and emphasis, inviting quick thinking and a positive response. Sample B is also informative and engaging, but leans more towards a comprehensive and serious approach. However, Sample A's phrasing makes it seem more colloquial and playful.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1160) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Multilingual individuals code switch between languages as a part of a complex communication process. However, most computational studies have examined only one or a handful of contextual factors predictive of switching. Here, we examine Naija-English code switching in a rich contextual environment to understand the social and topical factors eliciting a switch. We introduce a new corpus of 330K articles and accompanying 389K comments labeled for code switching behavior. In modeling whether a comment will switch, we show that topic-driven variation, tribal affiliation, emotional valence, and audience design all play complementary roles in behavior.\n",
            "\n",
            "[Sample A]:\n",
            "Analysis of Sociolinguistic Predictors of Naija-English Code-switching in a Large Corpus of Nigerian Online Interactions\n",
            "\n",
            "[Sample B]:\n",
            "Wetin dey with these comments? Modeling Sociolinguistic Factors Affecting Code-switching Behavior in Nigerian Online Discussions\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are relevant and concise, but Sample B is more engaging with its playful tone. It uses a fun wordplay with \"wetin\" to engage the reader's curiosity. It also uses \"wetin\" in a context that is relevant to the study, making it more relatable to the audience.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1165) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We investigate how novel English-derived words (anglicisms) are used in a Germanlanguage Internet hip hop forum, and what factors contribute to their uptake.\n",
            "\n",
            "[Sample A]:\n",
            "Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum\n",
            "\n",
            "[Sample B]:\n",
            "A Study of the Use and Adoption Factors of English-Derived Words in a German-Language Hip Hop Internet Forum\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Samples are equally fun with the added bonus of interesting topic. Both effectively summarize the research without making overly direct comments on humor that could be misleading.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1170) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.\n",
            "\n",
            "[Sample A]:\n",
            "A Generalized Reparameterized Gradient Estimator with Reduced Variance for Variational Inference\n",
            "\n",
            "[Sample B]:\n",
            "Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and relatable, making it a more fun title.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1175) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.\n",
            "\n",
            "[Sample A]:\n",
            "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\n",
            "\n",
            "[Sample B]:\n",
            "Adversarial Reward Learning for Improving Visual Storytelling through Implicit Reward Functions\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A emphasizes the lack of perfs in automatic metrics and highlights the need for a better approach, making it more direct and to the point.\n",
            "Sample B suggests an improvement over state-of-the-art systems using human demonstrations and implicit reward functions, which is more abstract and could be perceived as a bit niche.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1180) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Precision computes how well a technique did what it was designed to do, and is not at issue here. Recall is in tended as a more general performance measure, yet R scores are difficult to interpret due in part to varying methods of calculating T. T includes only the pronouns that were included in the s tudy rather than all p ronouns in the data set. But since different studies consider different sorts of pronouns to be in scope, R scores f rom different studies are difficult to compare. Also, since the pronouns in scope for a s tudy might represent a large or small percentage of the pronouns in the corpus, R reveals little about a technique's utility for the general problem of p ronoun resolution. This paper proposes a new report ing format and a new performance measure to supplement R and P. Pronoun resolution studies differ in m an y respects, such as the method of calculating C and the under lying semantic assumptions, and this proposal does not address ways to make the studies themselves more consistent (for discussion of these issues, see Walker 1989, van Deemter and Kibble 1999, Mitkov 2000). Instead, we propose a report ing format that clarifies the details of a s tudy 's test data (especially those details that tend to differ be tween studies) and explicitly derives the numbers used to compute performance measures.\n",
            "\n",
            "[Sample A]:\n",
            "Standardized Reporting Frameworks and Performance Metrics for Pronoun Resolution Studies\n",
            "\n",
            "[Sample B]:\n",
            "The Uncommon Denominator: A Proposal for Consistent Reporting of Pronoun Resolution Results\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "The Abstract discusses the limitations of existing methods (R and P) for reporting and evaluating pronoun resolution studies, suggesting a new approach with a standardized reporting framework and performance metrics. The proposed framework aims to address issues with study consistency, different methods for calculating metrics, and variable scope of pronouns in studies. The new format can be seen as clarifying details and deriving numbers for metrics, which makes it more concrete compared to the \"standardized reporting\" and \"proposed\" nature of the old frameworks.\n",
            "\n",
            "Sample B is better at conveying the paper's primary message, which is about a new proposal for a standardized reporting format to bring consistency and clarity to pronoun resolution studies. This directly aligns with the abstract's content.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1185) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Social media users often make explicit predictions about upcoming events. Such statements vary in the degree of certainty the author expresses toward the outcome: \"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may win\" or \"No way Leonardo wins!\". Can popular beliefs on social media predict who will win? To answer this question, we build a corpus of tweets annotated for veridicality on which we train a log-linear classifier that detects positive veridicality with high precision. We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users' explicit predictions. Our method for forecasting winners is fully automated, relying only on a set of contenders as input. It requires no training data of past outcomes and outperforms sentiment and tweet volume baselines on a broad range of contest prediction tasks. We further demonstrate how our approach can be used to measure the reliability of individual accounts' predictions and retrospectively identify surprise outcomes.\n",
            "\n",
            "[Sample A]:\n",
            "\"i have a feeling trump will win..................\": Forecasting Winners and Losers from User Predictions on Twitter\n",
            "\n",
            "[Sample B]:\n",
            "Automated Prediction of Event Outcomes Using Veridicality-Annotated Tweets and User Beliefs Aggregation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more concise and to-the-point.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1190) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We describe Talk'n'Travel, a spoken dialogue language system for making air travel plans over the telephone. Talk'n'Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English. The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met.\n",
            "\n",
            "[Sample A]:\n",
            "A Plan-Based Mixed-Initiative Spoken Dialogue System for Telephone-Based Air Travel Planning\n",
            "\n",
            "[Sample B]:\n",
            "Talk'n'Travel: A Conversational System for Air Travel Planning\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more funnier because it uses less formal language and has a more casual tone compared to Sample A, which is more formal and technical.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1195) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "This paper proposes a computational approach for analysis of strokes in line drawings by artists. We aim at developing an AI methodology that facilitates attribution of drawings of unknown authors in a way that is not easy to be deceived by forged art. The methodology used is based on quantifying the characteristics of individual strokes in drawings. We propose a novel algorithm for segmenting individual strokes. We propose an approach that combines different hand-crafted and learned features for the task of quantifying stroke characteristics. We experimented with a dataset of 300 digitized drawings with over 80 thousands strokes. The collection mainly consisted of drawings of Pablo Picasso, Henry Matisse, and Egon Schiele, besides a small number of representative works of other artists. The experiments shows that the proposed methodology can classify individual strokes with accuracy 70%-90%, and aggregate over drawings with accuracy above 80%, while being robust to be deceived by fakes.\n",
            "\n",
            "[Sample A]:\n",
            "A Computational Approach for Stroke-Level Analysis to Attribute and Authenticate Artist Drawings Using AI\n",
            "\n",
            "[Sample B]:\n",
            "Picasso, Matisse, or a Fake? Automated Analysis of Drawings at the Stroke Level for Attribution and Authentication\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more intriguing because it challenges the viewer to figure out which is the real artist based on their analysis of the stroke-level attributions. \"A Computational Approach for Stroke-Level Analysis to Attribute and Authenticate Artist Drawings Using AI\" is straightforward and neutral in its tone.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1200) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.\n",
            "\n",
            "[Sample A]:\n",
            "An Empirical Analysis of Zero-Shot and Few-Shot Language Transfer Limitations Using Massively Multilingual Transformers\n",
            "\n",
            "[Sample B]:\n",
            "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more descriptive and engaging, using the phrase \"Hero\", while Sample B is straightforward but less interesting.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1205) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’. To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’. Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.\n",
            "\n",
            "[Sample A]:\n",
            "Methods for Interpreting Scalar Modifier Meanings in Question-Answer Pairs Using Web-Based Data and Human Judgments\n",
            "\n",
            "[Sample B]:\n",
            "\"Was It Good? It Was Provocative.\" Learning the Meaning of Scalar Adjectives\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more funnier because it uses a humorous phrase \"Provocative\" to introduce the topic, making the abstract sound more engaging and interesting.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1210) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "We propose a general, yet simple patch that can be applied to existing regularizationbased continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier’s output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier’s output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and significantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL.\n",
            "\n",
            "[Sample A]:\n",
            "CPR: Classifier-Projection Regularization for Continual Learning\n",
            "\n",
            "[Sample B]:\n",
            "A Regularization Approach Using Classifier Output Entropy Projection to Improve Continual Learning Performance\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is straightforward and concise, emphasizing the key concept of the method. Sample B is more elaborate, building a connection between entropy projection and KL divergence, and discussing the theoretical implications. However, the latter explanation is somewhat academic and might not be as immediately engaging to a general audience.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1215) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "In order to determine argument structure in text, one must understand how individual components of the overall argument are linked. This work presents the first neural network-based approach to link extraction in argument mining. Specifically, we propose a novel architecture that applies Pointer Network sequence-to-sequence attention modeling to structural prediction in discourse parsing tasks. We then develop a joint model that extends this architecture to simultaneously address the link extraction task and the classification of argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models. Furthermore, our results demonstrate that jointly optimizing for both tasks is crucial for high performance.\n",
            "\n",
            "[Sample A]:\n",
            "Here's My Point: Joint Pointer Architecture for Argument Mining\n",
            "\n",
            "[Sample B]:\n",
            "A Joint Neural Network Model for Link Extraction and Component Classification in Argument Mining Using Pointer Networks\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more engaging and captures attention through humor. Sample B might be more direct and formal, which might fit better into a professional context. Sample A uses alliteration (\"Joint Pointer Architecture\") and wordplay (\"My Point\"). Sample B is straightforward but effective.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1220) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Curriculum learning is a technique to improve a model performance and generalization based on the idea that easy samples should be presented before difficult ones during training. While it is generally complex to estimate a priori the difficulty of a given sample, recent works have shown that curriculum learning can be formulated dynamically in a self-supervised manner. The key idea is to somehow estimate the importance (or weight) of each sample directly during training based on the observation that easy and hard samples behave differently and can therefore be separated. However, these approaches are usually limited to a specific task (e.g., classification) and require extra data annotations, layers or parameters as well as a dedicated training procedure. We propose instead a simple and generic method that can be applied to a variety of losses and tasks without any change in the learning procedure. It consists in appending a novel loss function on top of any existing task loss, hence its name: the SuperLoss. Its main effect is to automatically downweight the contribution of samples with a large loss, i.e. hard samples, effectively mimicking the core principle of curriculum learning. As a side effect, we show that our loss prevents the memorization of noisy samples, making it possible to train from noisy data even with non-robust loss functions. Experimental results on image classification, regression, object detection and image retrieval demonstrate consistent gain, particularly in the presence of noise.\n",
            "\n",
            "[Sample A]:\n",
            "A Loss Function for Adaptive Sample Weighting in Curriculum Learning to Improve Robustness to Noisy Data Across Vision Tasks\n",
            "\n",
            "[Sample B]:\n",
            "SuperLoss: A Generic Loss for Robust Curriculum Learning\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is the more interesting title as it highlights the novel feature of automatically downweighting hard samples, which is the core principle of curriculum learning. However, it is not as catchy as Sample B. Both titles are well-suited for research.\n",
            "\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1225) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.\n",
            "\n",
            "[Sample A]:\n",
            "A Framework to Reduce Noise in Distantly Supervised Relation Extraction Using Contrastive Instance Learning Compared to Multi-Instance Learning\n",
            "\n",
            "[Sample B]:\n",
            "CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample A is more complex, uses more technical terms, and the comparison is clearer. Sample B is simpler, uses less technical language, and directly refers to the framework it proposes.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1230) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "The context in which a name appears in a caption provides powerful cues as to who is depicted in the associated image. We obtain 44,773 face images, using a face detector, from approximately half a million captioned news images and automatically link names, obtained using a named entity recognizer, with these faces. A simple clustering method can produce fair results. We improve these results significantly by combining the clustering process with a model of the probability that an individual is depicted given its context. Once the labeling procedure is over, we have an accurately labeled set of faces, an appearance model for each individual depicted, and a natural language model that can produce accurate results on captions in isolation.\n",
            "\n",
            "[Sample A]:\n",
            "Who's In the Picture\n",
            "\n",
            "[Sample B]:\n",
            "Context-Aware Automated Labeling and Clustering of Faces in Captioned News Images Using Named Entity Recognition\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Sample B is more engaging and informative, highlighting the context-aware approach and the use of a named entity recognizer, making it likely more compelling for readers.\n",
            "</notes>\n",
            "<judgement>B</judgement>\n",
            "======================================================================\n",
            "\n",
            "==================== Training Step Log (Print Step 1235) ====================\n",
            "\n",
            "--- Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.\n",
            "\n",
            "[Sample A]:\n",
            "A Non-Euclidean Approach Using Product Riemannian Manifolds for Dynamic Temporal Knowledge Graph Embedding and Completion\n",
            "\n",
            "[Sample B]:\n",
            "DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "\n",
            "\n",
            "--- Generated Response [0] ---\n",
            "<notes>\n",
            "Both titles are similar in message and academic tone, but Sample A is more concise and straightforward, while Sample B is more explicit and descriptive. Sample A is therefore more engaging and inviting for readers who are curious about the approach and its potential benefits.\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "======================================================================\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.save_model() # Save LoRA adapter\n",
        "print(f\"LoRA adapters saved to {training_args.output_dir}\")\n",
        "\n",
        "# Prepare a test example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "siAewNryYrRU",
        "outputId": "88cfca1a-8122-43f9-c2d5-da342c61777c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA adapters saved to outputs_title_eval_kalomaze_corrected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Code"
      ],
      "metadata": {
        "id": "83uFWNQIWuXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "from tqdm.notebook import tqdm # For progress bar\n",
        "import re\n",
        "import pandas as pd # For displaying results later\n",
        "\n",
        "# --- Make sure training is finished before running this ---\n",
        "# trainer.train() call should be completed above this section.\n",
        "\n",
        "print(\"\\n--- Starting Manual Evaluation on Eval Dataset ---\")\n",
        "\n",
        "# --- Reuse Stopping Criteria Setup from Testing ---\n",
        "# (Ensure this is defined/available in your notebook scope)\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __init__(self, stop_token_ids_list):\n",
        "        # Ensure stop_token_ids_list is a list of tensors\n",
        "        self.stop_token_ids_list = stop_token_ids_list\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        # Check if the *end* of the generated sequence matches any of the stop sequences\n",
        "        for stop_ids in self.stop_token_ids_list:\n",
        "            if input_ids.shape[1] >= stop_ids.shape[0]:\n",
        "                if torch.eq(input_ids[0, -stop_ids.shape[0]:], stop_ids).all():\n",
        "                    return True # Stop generation\n",
        "        return False # Continue generation\n",
        "\n",
        "# Tokenize the desired stop string '</judgement>'\n",
        "stop_string = \"</judgement>\"\n",
        "# Ensure tokenizer and model are on the same device\n",
        "device = model.device # Get device from model\n",
        "stop_token_ids = tokenizer.encode(stop_string, add_special_tokens=False, return_tensors='pt').to(device)\n",
        "\n",
        "# Also include the standard EOS token ID tensor\n",
        "eos_token_id_tensor = torch.tensor([[tokenizer.eos_token_id]], device=device)\n",
        "\n",
        "# Create the stopping criteria list\n",
        "# Pass a list containing tensors for each stop sequence\n",
        "stop_criteria = StoppingCriteriaList([StopOnTokens([stop_token_ids[0], eos_token_id_tensor[0]])])\n",
        "\n",
        "# --- Helper Function to Extract Tags (ensure this matches your testing code) ---\n",
        "# --- Helper Function to Extract Tags (Corrected Regex) ---\n",
        "def extract_tags(text):\n",
        "    # Regex for <notes> seems fine if it captures until </notes>\n",
        "    notes_match = re.search(r\"<notes>(.*?)</notes>\", text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    # --- Corrected Regex for <judgement> ---\n",
        "    # Make the closing </judgement> tag optional\n",
        "    # It looks for <judgement>, optional whitespace, [AB], optional whitespace, then the optional </judgement>\n",
        "    judgement_match = re.search(r\"<judgement>\\s*([AB])\\s*(?:</judgement>)?\", text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    notes = notes_match.group(1).strip() if notes_match else None\n",
        "    judgement = judgement_match.group(1).strip() if judgement_match else None # Still strip whitespace around A/B\n",
        "    return notes, judgement\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "# ... (rest of your evaluation code remains the same)\n",
        "# --- Evaluation Loop ---\n",
        "results = []\n",
        "correct_judgements = 0\n",
        "format_matches = 0\n",
        "total_evaluated = 0\n",
        "\n",
        "# Regex for format checking (adjust if needed)\n",
        "format_pattern = r\"^\\s*<notes>.*?</notes>\\s*<judgement>\\s*[AB]\\s*</judgement>\\s*$\"\n",
        "\n",
        "# Ensure model is in evaluation mode (disables dropout, etc.)\n",
        "model.eval()\n",
        "\n",
        "# Use torch.no_grad() to save memory and computation during inference\n",
        "with torch.no_grad():\n",
        "    # Loop through the evaluation dataset with a progress bar\n",
        "    for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "        total_evaluated += 1\n",
        "        inference_prompt_chat = item['prompt'] # Get pre-formatted prompt\n",
        "        inference_prompt_text = tokenizer.apply_chat_template(\n",
        "            inference_prompt_chat,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True # Add the prompt for generation\n",
        "        )\n",
        "        target_choice = item['target_choice'] # Ground truth ('A' or 'B')\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            inference_prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=1010 # Use same max_prompt_length as training\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate response\n",
        "        try:\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=178, # Use same max_completion_length\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                # eos_token_id=tokenizer.eos_token_id, # EOS handled by stopping criteria\n",
        "                stopping_criteria=stop_criteria,\n",
        "                do_sample=True, # Use same sampling params as testing for consistency\n",
        "                temperature=0.6,\n",
        "                top_p=0.9,\n",
        "            )\n",
        "            # Decode generated part\n",
        "            generated_text = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "            # Optional: Clean the stop string if present at the end\n",
        "            if generated_text.endswith(stop_string):\n",
        "                 generated_text = generated_text[:-len(stop_string)]\n",
        "\n",
        "            # Analyze the generated text\n",
        "            notes, judgement = extract_tags(generated_text)\n",
        "            is_correct = False\n",
        "            matches_format = bool(re.match(format_pattern, generated_text, re.DOTALL | re.IGNORECASE))\n",
        "\n",
        "            if judgement and judgement == target_choice:\n",
        "                correct_judgements += 1\n",
        "                is_correct = True\n",
        "\n",
        "            if matches_format:\n",
        "                 format_matches += 1\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                \"prompt_text\": inference_prompt_text,\n",
        "                \"generated_text\": generated_text,\n",
        "                \"extracted_notes\": notes,\n",
        "                \"extracted_judgement\": judgement,\n",
        "                \"target_choice\": target_choice,\n",
        "                \"is_correct\": is_correct,\n",
        "                \"matches_format\": matches_format\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing item {total_evaluated}: {e}\")\n",
        "            # Store minimal info for errored items\n",
        "            results.append({\n",
        "                \"prompt_text\": inference_prompt_text,\n",
        "                \"generated_text\": f\"ERROR: {e}\",\n",
        "                \"extracted_notes\": None,\n",
        "                \"extracted_judgement\": None,\n",
        "                \"target_choice\": target_choice,\n",
        "                \"is_correct\": False,\n",
        "                \"matches_format\": False\n",
        "            })\n",
        "\n",
        "\n",
        "# --- Calculate and Print Final Metrics ---\n",
        "print(\"\\n--- Evaluation Summary ---\")\n",
        "if total_evaluated > 0:\n",
        "    accuracy = (correct_judgements / total_evaluated) * 100\n",
        "    format_match_rate = (format_matches / total_evaluated) * 100\n",
        "    print(f\"Total Items Evaluated: {total_evaluated}\")\n",
        "    print(f\"Correct Judgements: {correct_judgements}\")\n",
        "    print(f\"Judgement Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Format Matches: {format_matches}\")\n",
        "    print(f\"Format Match Rate: {format_match_rate:.2f}%\")\n",
        "else:\n",
        "    print(\"No items were evaluated.\")\n",
        "\n",
        "# --- Optional: Display some results in a DataFrame ---\n",
        "print(\"\\n--- Sample Evaluation Results ---\")\n",
        "results_df = pd.DataFrame(results)\n",
        "# Display head and maybe some incorrect examples\n",
        "print(results_df.head())\n",
        "\n",
        "incorrect_samples = results_df[results_df['is_correct'] == False].head()\n",
        "if not incorrect_samples.empty:\n",
        "    print(\"\\n--- Sample Incorrect Judgements ---\")\n",
        "    for index, row in incorrect_samples.iterrows():\n",
        "        print(f\"\\nItem {index}:\")\n",
        "        # print(f\"Prompt:\\n{row['prompt_text']}\") # Can be very long\n",
        "        print(f\"Target: {row['target_choice']}\")\n",
        "        print(f\"Generated Judgement: {row['extracted_judgement']}\")\n",
        "        print(f\"Generated Text:\\n{row['generated_text']}\")\n",
        "else:\n",
        "     print(\"\\nNo incorrect judgements found in the sample.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1685,
          "referenced_widgets": [
            "cc4a60e41b6e44f592b7e0d9bf9ef766",
            "1cacb1a59e364097a01e53166fba7c8e",
            "d346bbff74a24dcf892979567d6d63ff",
            "b737a39835c04b58b6a6e2b364a93c5e",
            "e623280f08044cfe935eb880f94ba3d2",
            "2c5c1fa0560e4cb08c0fbe68189a7450",
            "f19d6e3045c5419e94720b0a7acf7fa0",
            "58af688595e34b2ebc3ca6258abd08bd",
            "991ef15ba63a4c599c3da1889ebe2a90",
            "d27f5906ba7348c689b8779928d885ee",
            "384ca2001bf64bd080fcb5e418693804"
          ]
        },
        "id": "Fsm0l4l6aiNo",
        "outputId": "75a20e2c-9e3c-44e7-fcd7-5ae24c972249"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Manual Evaluation on Eval Dataset ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/49 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc4a60e41b6e44f592b7e0d9bf9ef766"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Summary ---\n",
            "Total Items Evaluated: 49\n",
            "Correct Judgements: 30\n",
            "Judgement Accuracy: 61.22%\n",
            "Format Matches: 0\n",
            "Format Match Rate: 0.00%\n",
            "\n",
            "--- Sample Evaluation Results ---\n",
            "                                         prompt_text  \\\n",
            "0  <|im_start|>system\\n\\nREQUEST: You are an expe...   \n",
            "1  <|im_start|>system\\n\\nREQUEST: You are an expe...   \n",
            "2  <|im_start|>system\\n\\nREQUEST: You are an expe...   \n",
            "3  <|im_start|>system\\n\\nREQUEST: You are an expe...   \n",
            "4  <|im_start|>system\\n\\nREQUEST: You are an expe...   \n",
            "\n",
            "                                      generated_text  \\\n",
            "0  <notes>\\nThe title \"Reading Like HER: Human Re...   \n",
            "1  <notes>\\nThe title \"Improving the Stability of...   \n",
            "2  <notes>\\nSample A captures the essence of pred...   \n",
            "3  <notes>\\nThe title \"It's All in the Name: Miti...   \n",
            "4  <notes>\\nThe title \"Humor in Word Embeddings: ...   \n",
            "\n",
            "                                     extracted_notes extracted_judgement  \\\n",
            "0  The title \"Reading Like HER: Human Reading Ins...                   B   \n",
            "1  The title \"Improving the Stability of Generati...                   B   \n",
            "2  Sample A captures the essence of predicting as...                   A   \n",
            "3  The title \"It's All in the Name: Mitigating Ge...                   B   \n",
            "4  The title \"Humor in Word Embeddings: Cockamami...                   B   \n",
            "\n",
            "  target_choice  is_correct  matches_format  \n",
            "0             B        True           False  \n",
            "1             B        True           False  \n",
            "2             A        True           False  \n",
            "3             A       False           False  \n",
            "4             B        True           False  \n",
            "\n",
            "--- Sample Incorrect Judgements ---\n",
            "\n",
            "Item 3:\n",
            "Target: A\n",
            "Generated Judgement: B\n",
            "Generated Text:\n",
            "<notes>\n",
            "The title \"It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution\" is more catchy and intriguing because it highlights the focus on names and substitution techniques, making it seem more innovative and impactful compared to the broader scope of counterfactual data augmentation and its applications.</notes>\n",
            "\n",
            "<judgement>B\n",
            "\n",
            "Item 10:\n",
            "Target: A\n",
            "Generated Judgement: B\n",
            "Generated Text:\n",
            "<notes>\n",
            "The title \"XHate-999: Analyzing and Detecting Abusive Language Across Domains and Languages\" seems more engaging and concise. It directly states the purpose of the study and its scope, making it easier for readers to understand what the paper is about at a glance. The use of \"Analyzing and Detecting\" also adds a sense of action and urgency, which which might make it stand out from other titles.\n",
            "</notes>\n",
            "\n",
            "<judgement>B\n",
            "\n",
            "Item 11:\n",
            "Target: A\n",
            "Generated Judgement: B\n",
            "Generated Text:\n",
            "<notes>\n",
            "Acquiring numerical common sense involves understanding typical values associated with different contexts. This study explores how we can learn these norms by analyzing web data. Two techniques are proposed: estimating distributions of related numbers and identifying explicit judgments made by speakers. The effectiveness of both approaches is evaluated through experiments.\n",
            "</notes>\n",
            "\n",
            "</notes>\n",
            "<judgement>B\n",
            "\n",
            "Item 14:\n",
            "Target: A\n",
            "Generated Judgement: B\n",
            "Generated Text:\n",
            "<notes>\n",
            "The title \"Stay With Me\" is more relatable and catchy compared to the technical sounding title \"A Model and Algorithm.\" The former immediately evokes imagery and can grab readers' attention due to its playful tone.\n",
            "</notes>\n",
            "<judgement>B\n",
            "\n",
            "Item 19:\n",
            "Target: A\n",
            "Generated Judgement: B\n",
            "Generated Text:\n",
            "<notes>\n",
            "Galactic Dependencies introduces unique synthetic datasets, offering more diverse training resources compared to traditional approaches. The title highlights its innovative approach and practical applications without being overly technical.\n",
            "</notes>\n",
            "<judgement>B\n"
          ]
        }
      ]
    }
  ]
}