{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg5ysHy3zZ9u"
      },
      "outputs": [],
      "source": [
        "# !pip Install transformers\n",
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import random\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import torch"
      ],
      "metadata": {
        "id": "hEK_Ai_D0gY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"RoyVoy/GRPO-as-a-humour-Judge2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"RoyVoy/GRPO-as-a-humour-Judge2\")"
      ],
      "metadata": {
        "id": "nJW_q2ym2yyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
        "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
        "\n",
        "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
        "\n",
        "For example:\n",
        "\n",
        "JUDGE: <notes>\n",
        "Sample A is superior to Sample B... (example notes)\n",
        "</notes>\n",
        "<judgement>A</judgement>\n",
        "\n",
        "Now, it is your turn.\"\"\"\n"
      ],
      "metadata": {
        "id": "m_T2jco23gGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_title_AB_prompt(example):\n",
        "    original = example['original_title']\n",
        "    created = example['created_title']\n",
        "    target_label = example['target_label']\n",
        "    assignment = {}\n",
        "    target_choice = None\n",
        "\n",
        "    if random.random() < 0.5:\n",
        "\n",
        "        sample_a_content = original\n",
        "        sample_b_content = created\n",
        "        assignment['A'] = 'original_title'\n",
        "        assignment['B'] = 'created_title'\n",
        "\n",
        "        target_choice = 'B' if target_label == 'created_title' else 'A'\n",
        "    else:\n",
        "        sample_a_content = created\n",
        "        sample_b_content = original\n",
        "        assignment['A'] = 'created_title'\n",
        "        assignment['B'] = 'original_title'\n",
        "\n",
        "        target_choice = 'A' if target_label == 'created_title' else 'B'\n",
        "\n",
        "    user_content = f\"\"\"Abstract:\n",
        "{example['abstract']}\n",
        "\n",
        "[Sample A]:\n",
        "{sample_a_content}\n",
        "\n",
        "[Sample B]:\n",
        "{sample_b_content}\n",
        "\n",
        "JUDGE:\"\"\"\n",
        "\n",
        "\n",
        "    prompt_list = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ]\n",
        "\n",
        "\n",
        "    example['prompt'] = prompt_list\n",
        "    example['assignment'] = assignment # Record which title is A vs B\n",
        "    example['target_choice'] = target_choice # Record the 'correct' letter (A or B)\n",
        "\n",
        "    return example\n"
      ],
      "metadata": {
        "id": "112GkQxo3YE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __init__(self, stop_token_ids):\n",
        "\n",
        "        self.stop_token_ids = stop_token_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in self.stop_token_ids:\n",
        "            if input_ids.shape[1] >= stop_ids.shape[0]:\n",
        "                if torch.eq(input_ids[0, -stop_ids.shape[0]:], stop_ids).all():\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "test_example_raw = {\n",
        "    'abstract': \"Despite decades of research, the aerodynamic properties of common breakfast pastries remain poorly understood. This study utilizes computational fluid dynamics (CFD) to model airflow around a glazed donut at various subsonic speeds. Results indicate significant flow separation and unexpected vortex shedding, suggesting non-trivial lift generation potential under specific icing conditions.\",\n",
        "    'original_title': \"CFD Analysis of Airflow Around a Toroidal Pastry\",\n",
        "    'created_title': \"Soaring Donuts: An Aerodynamic Investigation\",\n",
        "    'humor_category': \"Absurdity\",\n",
        "    'target_label': 'created_title'\n",
        "}\n",
        "\n",
        "formatted_test_data = format_title_AB_prompt(test_example_raw)\n",
        "inference_prompt_chat = formatted_test_data['prompt']\n",
        "inference_prompt_text = tokenizer.apply_chat_template(inference_prompt_chat, tokenize=False, add_generation_prompt=True)\n",
        "test_assignment = formatted_test_data['assignment']\n",
        "test_target_choice = formatted_test_data['target_choice']\n",
        "\n",
        "\n",
        "print(\"\\n--- Test Prompt ---\")\n",
        "print(inference_prompt_text)\n",
        "print(f\"Assignment: {test_assignment}\")\n",
        "print(f\"Correct Choice Should Be: {test_target_choice}\")\n",
        "print(\"--- Generating Response ---\")\n",
        "\n",
        "\n",
        "stop_string = \"</judgement>\"\n",
        "stop_token_ids = tokenizer.encode(stop_string, add_special_tokens=False, return_tensors='pt').to(model.device)\n",
        "\n",
        "\n",
        "eos_token_id_tensor = torch.tensor([[tokenizer.eos_token_id]], device=model.device) # Wrap in list and tensor\n",
        "\n",
        "\n",
        "stop_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids)])\n",
        "\n",
        "inputs = tokenizer(inference_prompt_text, return_tensors=\"pt\", truncation=True, max_length=1010).to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=173,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        stopping_criteria=stop_criteria,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "# --- Rest of your analysis code ---\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "\n",
        "if generated_text.endswith(stop_string):\n",
        "    generated_text = generated_text[:-len(stop_string)]\n",
        "print(generated_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmv2aQoZ3ILN",
        "outputId": "51539632-6f65-445e-e5f0-815dc229f96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Prompt ---\n",
            "<|im_start|>system\n",
            "\n",
            "REQUEST: You are an expert academic editor comparing two potential titles for a research abstract.\n",
            "You must judge which of the two titles, Sample A or Sample B, is Funnier, considering the provided abstract.\n",
            "\n",
            "Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking.\n",
            "\n",
            "For example:\n",
            "\n",
            "JUDGE: <notes>\n",
            "Sample A is superior to Sample B... (example notes)\n",
            "</notes>\n",
            "<judgement>A</judgement>\n",
            "\n",
            "Now, it is your turn.<|im_end|>\n",
            "<|im_start|>user\n",
            "Abstract:\n",
            "Despite decades of research, the aerodynamic properties of common breakfast pastries remain poorly understood. This study utilizes computational fluid dynamics (CFD) to model airflow around a glazed donut at various subsonic speeds. Results indicate significant flow separation and unexpected vortex shedding, suggesting non-trivial lift generation potential under specific icing conditions.\n",
            "\n",
            "[Sample A]:\n",
            "CFD Analysis of Airflow Around a Toroidal Pastry\n",
            "\n",
            "[Sample B]:\n",
            "Soaring Donuts: An Aerodynamic Investigation\n",
            "\n",
            "JUDGE:<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "Assignment: {'A': 'original_title', 'B': 'created_title'}\n",
            "Correct Choice Should Be: B\n",
            "--- Generating Response ---\n",
            "\n",
            "--- Generated Response ---\n",
            "<notes>\n",
            "Sample B captures the playful tone better by using \"soaring\" instead of \"flowing,\" making it more engaging.\n",
            "</notes>\n",
            "<judgement>B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_Jj9a8s3b0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}